// Generation parameters:
//   aws-sdk-js definitions from v2.1060.0
//   AWS service UID: rekognition-2016-06-27
//   code generation: v0.3
//   generated at: 2022-09-17

// Originally served at https://aws-api.deno.dev/v0.3/services/rekognition.ts

// Autogenerated API client for: Amazon Rekognition

import * as Base64 from "https://deno.land/std@0.120.0/encoding/base64.ts";
import * as client from "https://deno.land/x/aws_api@v0.6.0/client/common.ts";
import * as cmnP from "https://deno.land/x/aws_api@v0.6.0/encoding/common.ts";
import * as jsonP from "https://deno.land/x/aws_api@v0.6.0/encoding/json.ts";
export function serializeBlob(input: string | Uint8Array | null | undefined) {
  if (input == null) return input;
  return Base64.encode(input);
}

export class Rekognition {
  #client: client.ServiceClient;
  constructor(apiFactory: client.ApiFactory) {
    this.#client = apiFactory.buildServiceClient(Rekognition.ApiMetadata);
  }

  static ApiMetadata: client.ApiMetadata = {
    "apiVersion": "2016-06-27",
    "endpointPrefix": "rekognition",
    "jsonVersion": "1.1",
    "protocol": "json",
    "serviceFullName": "Amazon Rekognition",
    "serviceId": "Rekognition",
    "signatureVersion": "v4",
    "targetPrefix": "RekognitionService",
    "uid": "rekognition-2016-06-27"
  };

  /** Compares a face in the _source_ input image with each of the 100 largest faces detected in the _target_ input image. */
  async compareFaces(
    params: CompareFacesRequest,
    opts: client.RequestOptions = {},
  ): Promise<CompareFacesResponse> {
    const body: jsonP.JSONObject = {
      SourceImage: fromImage(params["SourceImage"]),
      TargetImage: fromImage(params["TargetImage"]),
      SimilarityThreshold: params["SimilarityThreshold"],
      QualityFilter: params["QualityFilter"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "CompareFaces",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "SourceImageFace": toComparedSourceImageFace,
        "FaceMatches": [toCompareFacesMatch],
        "UnmatchedFaces": [toComparedFace],
        "SourceImageOrientationCorrection": (x: jsonP.JSONValue) => cmnP.readEnum<OrientationCorrection>(x),
        "TargetImageOrientationCorrection": (x: jsonP.JSONValue) => cmnP.readEnum<OrientationCorrection>(x),
      },
    }, await resp.json());
  }

  /** Creates a collection in an AWS Region. */
  async createCollection(
    params: CreateCollectionRequest,
    opts: client.RequestOptions = {},
  ): Promise<CreateCollectionResponse> {
    const body: jsonP.JSONObject = {
      CollectionId: params["CollectionId"],
      Tags: params["Tags"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "CreateCollection",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "StatusCode": "n",
        "CollectionArn": "s",
        "FaceModelVersion": "s",
      },
    }, await resp.json());
  }

  /** Creates a new Amazon Rekognition Custom Labels dataset. */
  async createDataset(
    params: CreateDatasetRequest,
    opts: client.RequestOptions = {},
  ): Promise<CreateDatasetResponse> {
    const body: jsonP.JSONObject = {
      DatasetSource: fromDatasetSource(params["DatasetSource"]),
      DatasetType: params["DatasetType"],
      ProjectArn: params["ProjectArn"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "CreateDataset",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "DatasetArn": "s",
      },
    }, await resp.json());
  }

  /** Creates a new Amazon Rekognition Custom Labels project. */
  async createProject(
    params: CreateProjectRequest,
    opts: client.RequestOptions = {},
  ): Promise<CreateProjectResponse> {
    const body: jsonP.JSONObject = {
      ProjectName: params["ProjectName"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "CreateProject",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "ProjectArn": "s",
      },
    }, await resp.json());
  }

  /** Creates a new version of a model and begins training. */
  async createProjectVersion(
    params: CreateProjectVersionRequest,
    opts: client.RequestOptions = {},
  ): Promise<CreateProjectVersionResponse> {
    const body: jsonP.JSONObject = {
      ProjectArn: params["ProjectArn"],
      VersionName: params["VersionName"],
      OutputConfig: fromOutputConfig(params["OutputConfig"]),
      TrainingData: fromTrainingData(params["TrainingData"]),
      TestingData: fromTestingData(params["TestingData"]),
      Tags: params["Tags"],
      KmsKeyId: params["KmsKeyId"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "CreateProjectVersion",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "ProjectVersionArn": "s",
      },
    }, await resp.json());
  }

  /** Creates an Amazon Rekognition stream processor that you can use to detect and recognize faces in a streaming video. */
  async createStreamProcessor(
    params: CreateStreamProcessorRequest,
    opts: client.RequestOptions = {},
  ): Promise<CreateStreamProcessorResponse> {
    const body: jsonP.JSONObject = {
      Input: fromStreamProcessorInput(params["Input"]),
      Output: fromStreamProcessorOutput(params["Output"]),
      Name: params["Name"],
      Settings: fromStreamProcessorSettings(params["Settings"]),
      RoleArn: params["RoleArn"],
      Tags: params["Tags"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "CreateStreamProcessor",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "StreamProcessorArn": "s",
      },
    }, await resp.json());
  }

  /** Deletes the specified collection. */
  async deleteCollection(
    params: DeleteCollectionRequest,
    opts: client.RequestOptions = {},
  ): Promise<DeleteCollectionResponse> {
    const body: jsonP.JSONObject = {
      CollectionId: params["CollectionId"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DeleteCollection",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "StatusCode": "n",
      },
    }, await resp.json());
  }

  /** Deletes an existing Amazon Rekognition Custom Labels dataset. */
  async deleteDataset(
    params: DeleteDatasetRequest,
    opts: client.RequestOptions = {},
  ): Promise<void> {
    const body: jsonP.JSONObject = {
      DatasetArn: params["DatasetArn"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DeleteDataset",
    });
    await resp.arrayBuffer(); // consume body without use
  }

  /** Deletes faces from a collection. */
  async deleteFaces(
    params: DeleteFacesRequest,
    opts: client.RequestOptions = {},
  ): Promise<DeleteFacesResponse> {
    const body: jsonP.JSONObject = {
      CollectionId: params["CollectionId"],
      FaceIds: params["FaceIds"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DeleteFaces",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "DeletedFaces": ["s"],
      },
    }, await resp.json());
  }

  /** Deletes an Amazon Rekognition Custom Labels project. */
  async deleteProject(
    params: DeleteProjectRequest,
    opts: client.RequestOptions = {},
  ): Promise<DeleteProjectResponse> {
    const body: jsonP.JSONObject = {
      ProjectArn: params["ProjectArn"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DeleteProject",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "Status": (x: jsonP.JSONValue) => cmnP.readEnum<ProjectStatus>(x),
      },
    }, await resp.json());
  }

  /** Deletes an Amazon Rekognition Custom Labels model. */
  async deleteProjectVersion(
    params: DeleteProjectVersionRequest,
    opts: client.RequestOptions = {},
  ): Promise<DeleteProjectVersionResponse> {
    const body: jsonP.JSONObject = {
      ProjectVersionArn: params["ProjectVersionArn"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DeleteProjectVersion",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "Status": (x: jsonP.JSONValue) => cmnP.readEnum<ProjectVersionStatus>(x),
      },
    }, await resp.json());
  }

  /** Deletes the stream processor identified by `Name`. */
  async deleteStreamProcessor(
    params: DeleteStreamProcessorRequest,
    opts: client.RequestOptions = {},
  ): Promise<void> {
    const body: jsonP.JSONObject = {
      Name: params["Name"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DeleteStreamProcessor",
    });
    await resp.arrayBuffer(); // consume body without use
  }

  /** Describes the specified collection. */
  async describeCollection(
    params: DescribeCollectionRequest,
    opts: client.RequestOptions = {},
  ): Promise<DescribeCollectionResponse> {
    const body: jsonP.JSONObject = {
      CollectionId: params["CollectionId"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DescribeCollection",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "FaceCount": "n",
        "FaceModelVersion": "s",
        "CollectionARN": "s",
        "CreationTimestamp": "d",
      },
    }, await resp.json());
  }

  /** Describes an Amazon Rekognition Custom Labels dataset. */
  async describeDataset(
    params: DescribeDatasetRequest,
    opts: client.RequestOptions = {},
  ): Promise<DescribeDatasetResponse> {
    const body: jsonP.JSONObject = {
      DatasetArn: params["DatasetArn"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DescribeDataset",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "DatasetDescription": toDatasetDescription,
      },
    }, await resp.json());
  }

  /** Lists and describes the versions of a model in an Amazon Rekognition Custom Labels project. */
  async describeProjectVersions(
    params: DescribeProjectVersionsRequest,
    opts: client.RequestOptions = {},
  ): Promise<DescribeProjectVersionsResponse> {
    const body: jsonP.JSONObject = {
      ProjectArn: params["ProjectArn"],
      VersionNames: params["VersionNames"],
      NextToken: params["NextToken"],
      MaxResults: params["MaxResults"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DescribeProjectVersions",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "ProjectVersionDescriptions": [toProjectVersionDescription],
        "NextToken": "s",
      },
    }, await resp.json());
  }

  /** Gets information about your Amazon Rekognition Custom Labels projects. */
  async describeProjects(
    params: DescribeProjectsRequest = {},
    opts: client.RequestOptions = {},
  ): Promise<DescribeProjectsResponse> {
    const body: jsonP.JSONObject = {
      NextToken: params["NextToken"],
      MaxResults: params["MaxResults"],
      ProjectNames: params["ProjectNames"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DescribeProjects",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "ProjectDescriptions": [toProjectDescription],
        "NextToken": "s",
      },
    }, await resp.json());
  }

  /** Provides information about a stream processor created by "CreateStreamProcessor". */
  async describeStreamProcessor(
    params: DescribeStreamProcessorRequest,
    opts: client.RequestOptions = {},
  ): Promise<DescribeStreamProcessorResponse> {
    const body: jsonP.JSONObject = {
      Name: params["Name"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DescribeStreamProcessor",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "Name": "s",
        "StreamProcessorArn": "s",
        "Status": (x: jsonP.JSONValue) => cmnP.readEnum<StreamProcessorStatus>(x),
        "StatusMessage": "s",
        "CreationTimestamp": "d",
        "LastUpdateTimestamp": "d",
        "Input": toStreamProcessorInput,
        "Output": toStreamProcessorOutput,
        "RoleArn": "s",
        "Settings": toStreamProcessorSettings,
      },
    }, await resp.json());
  }

  /** Detects custom labels in a supplied image by using an Amazon Rekognition Custom Labels model. */
  async detectCustomLabels(
    params: DetectCustomLabelsRequest,
    opts: client.RequestOptions = {},
  ): Promise<DetectCustomLabelsResponse> {
    const body: jsonP.JSONObject = {
      ProjectVersionArn: params["ProjectVersionArn"],
      Image: fromImage(params["Image"]),
      MaxResults: params["MaxResults"],
      MinConfidence: params["MinConfidence"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DetectCustomLabels",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "CustomLabels": [toCustomLabel],
      },
    }, await resp.json());
  }

  /** Detects faces within an image that is provided as input. */
  async detectFaces(
    params: DetectFacesRequest,
    opts: client.RequestOptions = {},
  ): Promise<DetectFacesResponse> {
    const body: jsonP.JSONObject = {
      Image: fromImage(params["Image"]),
      Attributes: params["Attributes"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DetectFaces",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "FaceDetails": [toFaceDetail],
        "OrientationCorrection": (x: jsonP.JSONValue) => cmnP.readEnum<OrientationCorrection>(x),
      },
    }, await resp.json());
  }

  /** Detects instances of real-world entities within an image (JPEG or PNG) provided as input. */
  async detectLabels(
    params: DetectLabelsRequest,
    opts: client.RequestOptions = {},
  ): Promise<DetectLabelsResponse> {
    const body: jsonP.JSONObject = {
      Image: fromImage(params["Image"]),
      MaxLabels: params["MaxLabels"],
      MinConfidence: params["MinConfidence"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DetectLabels",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "Labels": [toLabel],
        "OrientationCorrection": (x: jsonP.JSONValue) => cmnP.readEnum<OrientationCorrection>(x),
        "LabelModelVersion": "s",
      },
    }, await resp.json());
  }

  /** Detects unsafe content in a specified JPEG or PNG format image. */
  async detectModerationLabels(
    params: DetectModerationLabelsRequest,
    opts: client.RequestOptions = {},
  ): Promise<DetectModerationLabelsResponse> {
    const body: jsonP.JSONObject = {
      Image: fromImage(params["Image"]),
      MinConfidence: params["MinConfidence"],
      HumanLoopConfig: fromHumanLoopConfig(params["HumanLoopConfig"]),
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DetectModerationLabels",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "ModerationLabels": [toModerationLabel],
        "ModerationModelVersion": "s",
        "HumanLoopActivationOutput": toHumanLoopActivationOutput,
      },
    }, await resp.json());
  }

  /** Detects Personal Protective Equipment (PPE) worn by people detected in an image. */
  async detectProtectiveEquipment(
    params: DetectProtectiveEquipmentRequest,
    opts: client.RequestOptions = {},
  ): Promise<DetectProtectiveEquipmentResponse> {
    const body: jsonP.JSONObject = {
      Image: fromImage(params["Image"]),
      SummarizationAttributes: fromProtectiveEquipmentSummarizationAttributes(params["SummarizationAttributes"]),
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DetectProtectiveEquipment",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "ProtectiveEquipmentModelVersion": "s",
        "Persons": [toProtectiveEquipmentPerson],
        "Summary": toProtectiveEquipmentSummary,
      },
    }, await resp.json());
  }

  /** Detects text in the input image and converts it into machine-readable text. */
  async detectText(
    params: DetectTextRequest,
    opts: client.RequestOptions = {},
  ): Promise<DetectTextResponse> {
    const body: jsonP.JSONObject = {
      Image: fromImage(params["Image"]),
      Filters: fromDetectTextFilters(params["Filters"]),
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DetectText",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "TextDetections": [toTextDetection],
        "TextModelVersion": "s",
      },
    }, await resp.json());
  }

  /** Distributes the entries (images) in a training dataset across the training dataset and the test dataset for a project. */
  async distributeDatasetEntries(
    params: DistributeDatasetEntriesRequest,
    opts: client.RequestOptions = {},
  ): Promise<void> {
    const body: jsonP.JSONObject = {
      Datasets: params["Datasets"]?.map(x => fromDistributeDataset(x)),
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "DistributeDatasetEntries",
    });
    await resp.arrayBuffer(); // consume body without use
  }

  /** Gets the name and additional information about a celebrity based on their Amazon Rekognition ID. */
  async getCelebrityInfo(
    params: GetCelebrityInfoRequest,
    opts: client.RequestOptions = {},
  ): Promise<GetCelebrityInfoResponse> {
    const body: jsonP.JSONObject = {
      Id: params["Id"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "GetCelebrityInfo",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "Urls": ["s"],
        "Name": "s",
        "KnownGender": toKnownGender,
      },
    }, await resp.json());
  }

  /** Gets the celebrity recognition results for a Amazon Rekognition Video analysis started by "StartCelebrityRecognition". */
  async getCelebrityRecognition(
    params: GetCelebrityRecognitionRequest,
    opts: client.RequestOptions = {},
  ): Promise<GetCelebrityRecognitionResponse> {
    const body: jsonP.JSONObject = {
      JobId: params["JobId"],
      MaxResults: params["MaxResults"],
      NextToken: params["NextToken"],
      SortBy: params["SortBy"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "GetCelebrityRecognition",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobStatus": (x: jsonP.JSONValue) => cmnP.readEnum<VideoJobStatus>(x),
        "StatusMessage": "s",
        "VideoMetadata": toVideoMetadata,
        "NextToken": "s",
        "Celebrities": [toCelebrityRecognition],
      },
    }, await resp.json());
  }

  /** Gets the inappropriate, unwanted, or offensive content analysis results for a Amazon Rekognition Video analysis started by "StartContentModeration". */
  async getContentModeration(
    params: GetContentModerationRequest,
    opts: client.RequestOptions = {},
  ): Promise<GetContentModerationResponse> {
    const body: jsonP.JSONObject = {
      JobId: params["JobId"],
      MaxResults: params["MaxResults"],
      NextToken: params["NextToken"],
      SortBy: params["SortBy"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "GetContentModeration",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobStatus": (x: jsonP.JSONValue) => cmnP.readEnum<VideoJobStatus>(x),
        "StatusMessage": "s",
        "VideoMetadata": toVideoMetadata,
        "ModerationLabels": [toContentModerationDetection],
        "NextToken": "s",
        "ModerationModelVersion": "s",
      },
    }, await resp.json());
  }

  /** Gets face detection results for a Amazon Rekognition Video analysis started by "StartFaceDetection". */
  async getFaceDetection(
    params: GetFaceDetectionRequest,
    opts: client.RequestOptions = {},
  ): Promise<GetFaceDetectionResponse> {
    const body: jsonP.JSONObject = {
      JobId: params["JobId"],
      MaxResults: params["MaxResults"],
      NextToken: params["NextToken"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "GetFaceDetection",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobStatus": (x: jsonP.JSONValue) => cmnP.readEnum<VideoJobStatus>(x),
        "StatusMessage": "s",
        "VideoMetadata": toVideoMetadata,
        "NextToken": "s",
        "Faces": [toFaceDetection],
      },
    }, await resp.json());
  }

  /** Gets the face search results for Amazon Rekognition Video face search started by "StartFaceSearch". */
  async getFaceSearch(
    params: GetFaceSearchRequest,
    opts: client.RequestOptions = {},
  ): Promise<GetFaceSearchResponse> {
    const body: jsonP.JSONObject = {
      JobId: params["JobId"],
      MaxResults: params["MaxResults"],
      NextToken: params["NextToken"],
      SortBy: params["SortBy"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "GetFaceSearch",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobStatus": (x: jsonP.JSONValue) => cmnP.readEnum<VideoJobStatus>(x),
        "StatusMessage": "s",
        "NextToken": "s",
        "VideoMetadata": toVideoMetadata,
        "Persons": [toPersonMatch],
      },
    }, await resp.json());
  }

  /** Gets the label detection results of a Amazon Rekognition Video analysis started by "StartLabelDetection". */
  async getLabelDetection(
    params: GetLabelDetectionRequest,
    opts: client.RequestOptions = {},
  ): Promise<GetLabelDetectionResponse> {
    const body: jsonP.JSONObject = {
      JobId: params["JobId"],
      MaxResults: params["MaxResults"],
      NextToken: params["NextToken"],
      SortBy: params["SortBy"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "GetLabelDetection",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobStatus": (x: jsonP.JSONValue) => cmnP.readEnum<VideoJobStatus>(x),
        "StatusMessage": "s",
        "VideoMetadata": toVideoMetadata,
        "NextToken": "s",
        "Labels": [toLabelDetection],
        "LabelModelVersion": "s",
      },
    }, await resp.json());
  }

  /** Gets the path tracking results of a Amazon Rekognition Video analysis started by "StartPersonTracking". */
  async getPersonTracking(
    params: GetPersonTrackingRequest,
    opts: client.RequestOptions = {},
  ): Promise<GetPersonTrackingResponse> {
    const body: jsonP.JSONObject = {
      JobId: params["JobId"],
      MaxResults: params["MaxResults"],
      NextToken: params["NextToken"],
      SortBy: params["SortBy"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "GetPersonTracking",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobStatus": (x: jsonP.JSONValue) => cmnP.readEnum<VideoJobStatus>(x),
        "StatusMessage": "s",
        "VideoMetadata": toVideoMetadata,
        "NextToken": "s",
        "Persons": [toPersonDetection],
      },
    }, await resp.json());
  }

  /** Gets the segment detection results of a Amazon Rekognition Video analysis started by "StartSegmentDetection". */
  async getSegmentDetection(
    params: GetSegmentDetectionRequest,
    opts: client.RequestOptions = {},
  ): Promise<GetSegmentDetectionResponse> {
    const body: jsonP.JSONObject = {
      JobId: params["JobId"],
      MaxResults: params["MaxResults"],
      NextToken: params["NextToken"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "GetSegmentDetection",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobStatus": (x: jsonP.JSONValue) => cmnP.readEnum<VideoJobStatus>(x),
        "StatusMessage": "s",
        "VideoMetadata": [toVideoMetadata],
        "AudioMetadata": [toAudioMetadata],
        "NextToken": "s",
        "Segments": [toSegmentDetection],
        "SelectedSegmentTypes": [toSegmentTypeInfo],
      },
    }, await resp.json());
  }

  /** Gets the text detection results of a Amazon Rekognition Video analysis started by "StartTextDetection". */
  async getTextDetection(
    params: GetTextDetectionRequest,
    opts: client.RequestOptions = {},
  ): Promise<GetTextDetectionResponse> {
    const body: jsonP.JSONObject = {
      JobId: params["JobId"],
      MaxResults: params["MaxResults"],
      NextToken: params["NextToken"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "GetTextDetection",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobStatus": (x: jsonP.JSONValue) => cmnP.readEnum<VideoJobStatus>(x),
        "StatusMessage": "s",
        "VideoMetadata": toVideoMetadata,
        "TextDetections": [toTextDetectionResult],
        "NextToken": "s",
        "TextModelVersion": "s",
      },
    }, await resp.json());
  }

  /** Detects faces in the input image and adds them to the specified collection. */
  async indexFaces(
    params: IndexFacesRequest,
    opts: client.RequestOptions = {},
  ): Promise<IndexFacesResponse> {
    const body: jsonP.JSONObject = {
      CollectionId: params["CollectionId"],
      Image: fromImage(params["Image"]),
      ExternalImageId: params["ExternalImageId"],
      DetectionAttributes: params["DetectionAttributes"],
      MaxFaces: params["MaxFaces"],
      QualityFilter: params["QualityFilter"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "IndexFaces",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "FaceRecords": [toFaceRecord],
        "OrientationCorrection": (x: jsonP.JSONValue) => cmnP.readEnum<OrientationCorrection>(x),
        "FaceModelVersion": "s",
        "UnindexedFaces": [toUnindexedFace],
      },
    }, await resp.json());
  }

  /** Returns list of collection IDs in your account. */
  async listCollections(
    params: ListCollectionsRequest = {},
    opts: client.RequestOptions = {},
  ): Promise<ListCollectionsResponse> {
    const body: jsonP.JSONObject = {
      NextToken: params["NextToken"],
      MaxResults: params["MaxResults"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "ListCollections",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "CollectionIds": ["s"],
        "NextToken": "s",
        "FaceModelVersions": ["s"],
      },
    }, await resp.json());
  }

  /** Lists the entries (images) within a dataset. */
  async listDatasetEntries(
    params: ListDatasetEntriesRequest,
    opts: client.RequestOptions = {},
  ): Promise<ListDatasetEntriesResponse> {
    const body: jsonP.JSONObject = {
      DatasetArn: params["DatasetArn"],
      ContainsLabels: params["ContainsLabels"],
      Labeled: params["Labeled"],
      SourceRefContains: params["SourceRefContains"],
      HasErrors: params["HasErrors"],
      NextToken: params["NextToken"],
      MaxResults: params["MaxResults"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "ListDatasetEntries",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "DatasetEntries": ["s"],
        "NextToken": "s",
      },
    }, await resp.json());
  }

  /** Lists the labels in a dataset. */
  async listDatasetLabels(
    params: ListDatasetLabelsRequest,
    opts: client.RequestOptions = {},
  ): Promise<ListDatasetLabelsResponse> {
    const body: jsonP.JSONObject = {
      DatasetArn: params["DatasetArn"],
      NextToken: params["NextToken"],
      MaxResults: params["MaxResults"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "ListDatasetLabels",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "DatasetLabelDescriptions": [toDatasetLabelDescription],
        "NextToken": "s",
      },
    }, await resp.json());
  }

  /** Returns metadata for faces in the specified collection. */
  async listFaces(
    params: ListFacesRequest,
    opts: client.RequestOptions = {},
  ): Promise<ListFacesResponse> {
    const body: jsonP.JSONObject = {
      CollectionId: params["CollectionId"],
      NextToken: params["NextToken"],
      MaxResults: params["MaxResults"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "ListFaces",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "Faces": [toFace],
        "NextToken": "s",
        "FaceModelVersion": "s",
      },
    }, await resp.json());
  }

  /** Gets a list of stream processors that you have created with "CreateStreamProcessor". */
  async listStreamProcessors(
    params: ListStreamProcessorsRequest = {},
    opts: client.RequestOptions = {},
  ): Promise<ListStreamProcessorsResponse> {
    const body: jsonP.JSONObject = {
      NextToken: params["NextToken"],
      MaxResults: params["MaxResults"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "ListStreamProcessors",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "NextToken": "s",
        "StreamProcessors": [toStreamProcessor],
      },
    }, await resp.json());
  }

  /** Returns a list of tags in an Amazon Rekognition collection, stream processor, or Custom Labels model. */
  async listTagsForResource(
    params: ListTagsForResourceRequest,
    opts: client.RequestOptions = {},
  ): Promise<ListTagsForResourceResponse> {
    const body: jsonP.JSONObject = {
      ResourceArn: params["ResourceArn"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "ListTagsForResource",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "Tags": x => jsonP.readMap(String, String, x),
      },
    }, await resp.json());
  }

  /** Returns an array of celebrities recognized in the input image. */
  async recognizeCelebrities(
    params: RecognizeCelebritiesRequest,
    opts: client.RequestOptions = {},
  ): Promise<RecognizeCelebritiesResponse> {
    const body: jsonP.JSONObject = {
      Image: fromImage(params["Image"]),
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "RecognizeCelebrities",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "CelebrityFaces": [toCelebrity],
        "UnrecognizedFaces": [toComparedFace],
        "OrientationCorrection": (x: jsonP.JSONValue) => cmnP.readEnum<OrientationCorrection>(x),
      },
    }, await resp.json());
  }

  /** For a given input face ID, searches for matching faces in the collection the face belongs to. */
  async searchFaces(
    params: SearchFacesRequest,
    opts: client.RequestOptions = {},
  ): Promise<SearchFacesResponse> {
    const body: jsonP.JSONObject = {
      CollectionId: params["CollectionId"],
      FaceId: params["FaceId"],
      MaxFaces: params["MaxFaces"],
      FaceMatchThreshold: params["FaceMatchThreshold"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "SearchFaces",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "SearchedFaceId": "s",
        "FaceMatches": [toFaceMatch],
        "FaceModelVersion": "s",
      },
    }, await resp.json());
  }

  /** For a given input image, first detects the largest face in the image, and then searches the specified collection for matching faces. */
  async searchFacesByImage(
    params: SearchFacesByImageRequest,
    opts: client.RequestOptions = {},
  ): Promise<SearchFacesByImageResponse> {
    const body: jsonP.JSONObject = {
      CollectionId: params["CollectionId"],
      Image: fromImage(params["Image"]),
      MaxFaces: params["MaxFaces"],
      FaceMatchThreshold: params["FaceMatchThreshold"],
      QualityFilter: params["QualityFilter"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "SearchFacesByImage",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "SearchedFaceBoundingBox": toBoundingBox,
        "SearchedFaceConfidence": "n",
        "FaceMatches": [toFaceMatch],
        "FaceModelVersion": "s",
      },
    }, await resp.json());
  }

  /** Starts asynchronous recognition of celebrities in a stored video. */
  async startCelebrityRecognition(
    params: StartCelebrityRecognitionRequest,
    opts: client.RequestOptions = {},
  ): Promise<StartCelebrityRecognitionResponse> {
    const body: jsonP.JSONObject = {
      Video: fromVideo(params["Video"]),
      ClientRequestToken: params["ClientRequestToken"],
      NotificationChannel: fromNotificationChannel(params["NotificationChannel"]),
      JobTag: params["JobTag"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "StartCelebrityRecognition",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobId": "s",
      },
    }, await resp.json());
  }

  /** Starts asynchronous detection of inappropriate, unwanted, or offensive content in a stored video. */
  async startContentModeration(
    params: StartContentModerationRequest,
    opts: client.RequestOptions = {},
  ): Promise<StartContentModerationResponse> {
    const body: jsonP.JSONObject = {
      Video: fromVideo(params["Video"]),
      MinConfidence: params["MinConfidence"],
      ClientRequestToken: params["ClientRequestToken"],
      NotificationChannel: fromNotificationChannel(params["NotificationChannel"]),
      JobTag: params["JobTag"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "StartContentModeration",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobId": "s",
      },
    }, await resp.json());
  }

  /** Starts asynchronous detection of faces in a stored video. */
  async startFaceDetection(
    params: StartFaceDetectionRequest,
    opts: client.RequestOptions = {},
  ): Promise<StartFaceDetectionResponse> {
    const body: jsonP.JSONObject = {
      Video: fromVideo(params["Video"]),
      ClientRequestToken: params["ClientRequestToken"],
      NotificationChannel: fromNotificationChannel(params["NotificationChannel"]),
      FaceAttributes: params["FaceAttributes"],
      JobTag: params["JobTag"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "StartFaceDetection",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobId": "s",
      },
    }, await resp.json());
  }

  /** Starts the asynchronous search for faces in a collection that match the faces of persons detected in a stored video. */
  async startFaceSearch(
    params: StartFaceSearchRequest,
    opts: client.RequestOptions = {},
  ): Promise<StartFaceSearchResponse> {
    const body: jsonP.JSONObject = {
      Video: fromVideo(params["Video"]),
      ClientRequestToken: params["ClientRequestToken"],
      FaceMatchThreshold: params["FaceMatchThreshold"],
      CollectionId: params["CollectionId"],
      NotificationChannel: fromNotificationChannel(params["NotificationChannel"]),
      JobTag: params["JobTag"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "StartFaceSearch",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobId": "s",
      },
    }, await resp.json());
  }

  /** Starts asynchronous detection of labels in a stored video. */
  async startLabelDetection(
    params: StartLabelDetectionRequest,
    opts: client.RequestOptions = {},
  ): Promise<StartLabelDetectionResponse> {
    const body: jsonP.JSONObject = {
      Video: fromVideo(params["Video"]),
      ClientRequestToken: params["ClientRequestToken"],
      MinConfidence: params["MinConfidence"],
      NotificationChannel: fromNotificationChannel(params["NotificationChannel"]),
      JobTag: params["JobTag"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "StartLabelDetection",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobId": "s",
      },
    }, await resp.json());
  }

  /** Starts the asynchronous tracking of a person's path in a stored video. */
  async startPersonTracking(
    params: StartPersonTrackingRequest,
    opts: client.RequestOptions = {},
  ): Promise<StartPersonTrackingResponse> {
    const body: jsonP.JSONObject = {
      Video: fromVideo(params["Video"]),
      ClientRequestToken: params["ClientRequestToken"],
      NotificationChannel: fromNotificationChannel(params["NotificationChannel"]),
      JobTag: params["JobTag"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "StartPersonTracking",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobId": "s",
      },
    }, await resp.json());
  }

  /** Starts the running of the version of a model. */
  async startProjectVersion(
    params: StartProjectVersionRequest,
    opts: client.RequestOptions = {},
  ): Promise<StartProjectVersionResponse> {
    const body: jsonP.JSONObject = {
      ProjectVersionArn: params["ProjectVersionArn"],
      MinInferenceUnits: params["MinInferenceUnits"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "StartProjectVersion",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "Status": (x: jsonP.JSONValue) => cmnP.readEnum<ProjectVersionStatus>(x),
      },
    }, await resp.json());
  }

  /** Starts asynchronous detection of segment detection in a stored video. */
  async startSegmentDetection(
    params: StartSegmentDetectionRequest,
    opts: client.RequestOptions = {},
  ): Promise<StartSegmentDetectionResponse> {
    const body: jsonP.JSONObject = {
      Video: fromVideo(params["Video"]),
      ClientRequestToken: params["ClientRequestToken"],
      NotificationChannel: fromNotificationChannel(params["NotificationChannel"]),
      JobTag: params["JobTag"],
      Filters: fromStartSegmentDetectionFilters(params["Filters"]),
      SegmentTypes: params["SegmentTypes"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "StartSegmentDetection",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobId": "s",
      },
    }, await resp.json());
  }

  /** Starts processing a stream processor. */
  async startStreamProcessor(
    params: StartStreamProcessorRequest,
    opts: client.RequestOptions = {},
  ): Promise<void> {
    const body: jsonP.JSONObject = {
      Name: params["Name"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "StartStreamProcessor",
    });
    await resp.arrayBuffer(); // consume body without use
  }

  /** Starts asynchronous detection of text in a stored video. */
  async startTextDetection(
    params: StartTextDetectionRequest,
    opts: client.RequestOptions = {},
  ): Promise<StartTextDetectionResponse> {
    const body: jsonP.JSONObject = {
      Video: fromVideo(params["Video"]),
      ClientRequestToken: params["ClientRequestToken"],
      NotificationChannel: fromNotificationChannel(params["NotificationChannel"]),
      JobTag: params["JobTag"],
      Filters: fromStartTextDetectionFilters(params["Filters"]),
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "StartTextDetection",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "JobId": "s",
      },
    }, await resp.json());
  }

  /** Stops a running model. */
  async stopProjectVersion(
    params: StopProjectVersionRequest,
    opts: client.RequestOptions = {},
  ): Promise<StopProjectVersionResponse> {
    const body: jsonP.JSONObject = {
      ProjectVersionArn: params["ProjectVersionArn"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "StopProjectVersion",
    });
    return jsonP.readObj({
      required: {},
      optional: {
        "Status": (x: jsonP.JSONValue) => cmnP.readEnum<ProjectVersionStatus>(x),
      },
    }, await resp.json());
  }

  /** Stops a running stream processor that was created by "CreateStreamProcessor". */
  async stopStreamProcessor(
    params: StopStreamProcessorRequest,
    opts: client.RequestOptions = {},
  ): Promise<void> {
    const body: jsonP.JSONObject = {
      Name: params["Name"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "StopStreamProcessor",
    });
    await resp.arrayBuffer(); // consume body without use
  }

  /** Adds one or more key-value tags to an Amazon Rekognition collection, stream processor, or Custom Labels model. */
  async tagResource(
    params: TagResourceRequest,
    opts: client.RequestOptions = {},
  ): Promise<void> {
    const body: jsonP.JSONObject = {
      ResourceArn: params["ResourceArn"],
      Tags: params["Tags"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "TagResource",
    });
    await resp.arrayBuffer(); // consume body without use
  }

  /** Removes one or more tags from an Amazon Rekognition collection, stream processor, or Custom Labels model. */
  async untagResource(
    params: UntagResourceRequest,
    opts: client.RequestOptions = {},
  ): Promise<void> {
    const body: jsonP.JSONObject = {
      ResourceArn: params["ResourceArn"],
      TagKeys: params["TagKeys"],
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "UntagResource",
    });
    await resp.arrayBuffer(); // consume body without use
  }

  /** Adds or updates one or more entries (images) in a dataset. */
  async updateDatasetEntries(
    params: UpdateDatasetEntriesRequest,
    opts: client.RequestOptions = {},
  ): Promise<void> {
    const body: jsonP.JSONObject = {
      DatasetArn: params["DatasetArn"],
      Changes: fromDatasetChanges(params["Changes"]),
    };
    const resp = await this.#client.performRequest({
      opts, body,
      action: "UpdateDatasetEntries",
    });
    await resp.arrayBuffer(); // consume body without use
  }

  // Resource State Waiters

  /**
   * Wait until the ProjectVersion training completes.
   * Checks state up to 360 times, 120 seconds apart (about 720 minutes max wait time).
   */
  async waitForProjectVersionTrainingCompleted(
    params: DescribeProjectVersionsRequest,
    opts: client.RequestOptions = {},
  ): Promise<DescribeProjectVersionsResponse> {
    const errMessage = 'ResourceNotReady: Resource is not in the state ProjectVersionTrainingCompleted';
    for (let i = 0; i < 360; i++) {
      const resp = await this.describeProjectVersions(params, opts);
      const field = resp?.ProjectVersionDescriptions?.flatMap(x => x?.Status);
      if (field?.every(x => x === "TRAINING_COMPLETED")) return resp;
      if (field?.some(x => x === "TRAINING_FAILED")) throw new Error(errMessage);
      await new Promise(r => setTimeout(r, 120000));
    }
    throw new Error(errMessage);
  }

  /**
   * Wait until the ProjectVersion is running.
   * Checks state up to 40 times, 30 seconds apart (about 20 minutes max wait time).
   */
  async waitForProjectVersionRunning(
    params: DescribeProjectVersionsRequest,
    opts: client.RequestOptions = {},
  ): Promise<DescribeProjectVersionsResponse> {
    const errMessage = 'ResourceNotReady: Resource is not in the state ProjectVersionRunning';
    for (let i = 0; i < 40; i++) {
      const resp = await this.describeProjectVersions(params, opts);
      const field = resp?.ProjectVersionDescriptions?.flatMap(x => x?.Status);
      if (field?.every(x => x === "RUNNING")) return resp;
      if (field?.some(x => x === "FAILED")) throw new Error(errMessage);
      await new Promise(r => setTimeout(r, 30000));
    }
    throw new Error(errMessage);
  }

}

// refs: 1 - tags: named, input
export interface CompareFacesRequest {
  /** The input image as base64-encoded bytes or an S3 object. */
  SourceImage: Image;
  /** The target image as base64-encoded bytes or an S3 object. */
  TargetImage: Image;
  /** The minimum level of confidence in the face matches that a match must meet to be included in the `FaceMatches` array. */
  SimilarityThreshold?: number | null;
  /** A filter that specifies a quality bar for how much filtering is done to identify faces. */
  QualityFilter?: QualityFilter | null;
}

// refs: 1 - tags: named, input
export interface CreateCollectionRequest {
  /** ID for the collection that you are creating. */
  CollectionId: string;
  /** A set of tags (key-value pairs) that you want to attach to the collection. */
  Tags?: { [key: string]: string | null | undefined } | null;
}

// refs: 1 - tags: named, input
export interface CreateDatasetRequest {
  /** The source files for the dataset. */
  DatasetSource?: DatasetSource | null;
  /** The type of the dataset. */
  DatasetType: DatasetType;
  /** The ARN of the Amazon Rekognition Custom Labels project to which you want to asssign the dataset. */
  ProjectArn: string;
}

// refs: 1 - tags: named, input
export interface CreateProjectRequest {
  /** The name of the project to create. */
  ProjectName: string;
}

// refs: 1 - tags: named, input
export interface CreateProjectVersionRequest {
  /** The ARN of the Amazon Rekognition Custom Labels project that manages the model that you want to train. */
  ProjectArn: string;
  /** A name for the version of the model. */
  VersionName: string;
  /** The Amazon S3 bucket location to store the results of training. */
  OutputConfig: OutputConfig;
  /** Specifies an external manifest that the services uses to train the model. */
  TrainingData?: TrainingData | null;
  /** Specifies an external manifest that the service uses to test the model. */
  TestingData?: TestingData | null;
  /** A set of tags (key-value pairs) that you want to attach to the model. */
  Tags?: { [key: string]: string | null | undefined } | null;
  /** The identifier for your AWS Key Management Service key (AWS KMS key). */
  KmsKeyId?: string | null;
}

// refs: 1 - tags: named, input
export interface CreateStreamProcessorRequest {
  /** Kinesis video stream stream that provides the source streaming video. */
  Input: StreamProcessorInput;
  /** Kinesis data stream stream to which Amazon Rekognition Video puts the analysis results. */
  Output: StreamProcessorOutput;
  /** An identifier you assign to the stream processor. */
  Name: string;
  /** Face recognition input parameters to be used by the stream processor. */
  Settings: StreamProcessorSettings;
  /** ARN of the IAM role that allows access to the stream processor. */
  RoleArn: string;
  /** A set of tags (key-value pairs) that you want to attach to the stream processor. */
  Tags?: { [key: string]: string | null | undefined } | null;
}

// refs: 1 - tags: named, input
export interface DeleteCollectionRequest {
  /** ID of the collection to delete. */
  CollectionId: string;
}

// refs: 1 - tags: named, input
export interface DeleteDatasetRequest {
  /** The ARN of the Amazon Rekognition Custom Labels dataset that you want to delete. */
  DatasetArn: string;
}

// refs: 1 - tags: named, input
export interface DeleteFacesRequest {
  /** Collection from which to remove the specific faces. */
  CollectionId: string;
  /** An array of face IDs to delete. */
  FaceIds: string[];
}

// refs: 1 - tags: named, input
export interface DeleteProjectRequest {
  /** The Amazon Resource Name (ARN) of the project that you want to delete. */
  ProjectArn: string;
}

// refs: 1 - tags: named, input
export interface DeleteProjectVersionRequest {
  /** The Amazon Resource Name (ARN) of the model version that you want to delete. */
  ProjectVersionArn: string;
}

// refs: 1 - tags: named, input
export interface DeleteStreamProcessorRequest {
  /** The name of the stream processor you want to delete. */
  Name: string;
}

// refs: 1 - tags: named, input
export interface DescribeCollectionRequest {
  /** The ID of the collection to describe. */
  CollectionId: string;
}

// refs: 1 - tags: named, input
export interface DescribeDatasetRequest {
  /** The Amazon Resource Name (ARN) of the dataset that you want to describe. */
  DatasetArn: string;
}

// refs: 1 - tags: named, input
export interface DescribeProjectVersionsRequest {
  /** The Amazon Resource Name (ARN) of the project that contains the models you want to describe. */
  ProjectArn: string;
  /** A list of model version names that you want to describe. */
  VersionNames?: string[] | null;
  /** If the previous response was incomplete (because there is more results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response. */
  NextToken?: string | null;
  /** The maximum number of results to return per paginated call. */
  MaxResults?: number | null;
}

// refs: 1 - tags: named, input
export interface DescribeProjectsRequest {
  /** If the previous response was incomplete (because there is more results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response. */
  NextToken?: string | null;
  /** The maximum number of results to return per paginated call. */
  MaxResults?: number | null;
  /** A list of the projects that you want Amazon Rekognition Custom Labels to describe. */
  ProjectNames?: string[] | null;
}

// refs: 1 - tags: named, input
export interface DescribeStreamProcessorRequest {
  /** Name of the stream processor for which you want information. */
  Name: string;
}

// refs: 1 - tags: named, input
export interface DetectCustomLabelsRequest {
  /** The ARN of the model version that you want to use. */
  ProjectVersionArn: string;
  Image: Image;
  /** Maximum number of results you want the service to return in the response. */
  MaxResults?: number | null;
  /** Specifies the minimum confidence level for the labels to return. */
  MinConfidence?: number | null;
}

// refs: 1 - tags: named, input
export interface DetectFacesRequest {
  /** The input image as base64-encoded bytes or an S3 object. */
  Image: Image;
  /** An array of facial attributes you want to be returned. */
  Attributes?: Attribute[] | null;
}

// refs: 1 - tags: named, input
export interface DetectLabelsRequest {
  /** The input image as base64-encoded bytes or an S3 object. */
  Image: Image;
  /** Maximum number of labels you want the service to return in the response. */
  MaxLabels?: number | null;
  /** Specifies the minimum confidence level for the labels to return. */
  MinConfidence?: number | null;
}

// refs: 1 - tags: named, input
export interface DetectModerationLabelsRequest {
  /** The input image as base64-encoded bytes or an S3 object. */
  Image: Image;
  /** Specifies the minimum confidence level for the labels to return. */
  MinConfidence?: number | null;
  /** Sets up the configuration for human evaluation, including the FlowDefinition the image will be sent to. */
  HumanLoopConfig?: HumanLoopConfig | null;
}

// refs: 1 - tags: named, input
export interface DetectProtectiveEquipmentRequest {
  /** The image in which you want to detect PPE on detected persons. */
  Image: Image;
  /** An array of PPE types that you want to summarize. */
  SummarizationAttributes?: ProtectiveEquipmentSummarizationAttributes | null;
}

// refs: 1 - tags: named, input
export interface DetectTextRequest {
  /** The input image as base64-encoded bytes or an Amazon S3 object. */
  Image: Image;
  /** Optional parameters that let you set the criteria that the text must meet to be included in your response. */
  Filters?: DetectTextFilters | null;
}

// refs: 1 - tags: named, input
export interface DistributeDatasetEntriesRequest {
  /** The ARNS for the training dataset and test dataset that you want to use. */
  Datasets: DistributeDataset[];
}

// refs: 1 - tags: named, input
export interface GetCelebrityInfoRequest {
  /** The ID for the celebrity. */
  Id: string;
}

// refs: 1 - tags: named, input
export interface GetCelebrityRecognitionRequest {
  /** Job identifier for the required celebrity recognition analysis. */
  JobId: string;
  /** Maximum number of results to return per paginated call. */
  MaxResults?: number | null;
  /** If the previous response was incomplete (because there is more recognized celebrities to retrieve), Amazon Rekognition Video returns a pagination token in the response. */
  NextToken?: string | null;
  /** Sort to use for celebrities returned in `Celebrities` field. */
  SortBy?: CelebrityRecognitionSortBy | null;
}

// refs: 1 - tags: named, input
export interface GetContentModerationRequest {
  /** The identifier for the inappropriate, unwanted, or offensive content moderation job. */
  JobId: string;
  /** Maximum number of results to return per paginated call. */
  MaxResults?: number | null;
  /** If the previous response was incomplete (because there is more data to retrieve), Amazon Rekognition returns a pagination token in the response. */
  NextToken?: string | null;
  /** Sort to use for elements in the `ModerationLabelDetections` array. */
  SortBy?: ContentModerationSortBy | null;
}

// refs: 1 - tags: named, input
export interface GetFaceDetectionRequest {
  /** Unique identifier for the face detection job. */
  JobId: string;
  /** Maximum number of results to return per paginated call. */
  MaxResults?: number | null;
  /** If the previous response was incomplete (because there are more faces to retrieve), Amazon Rekognition Video returns a pagination token in the response. */
  NextToken?: string | null;
}

// refs: 1 - tags: named, input
export interface GetFaceSearchRequest {
  /** The job identifer for the search request. */
  JobId: string;
  /** Maximum number of results to return per paginated call. */
  MaxResults?: number | null;
  /** If the previous response was incomplete (because there is more search results to retrieve), Amazon Rekognition Video returns a pagination token in the response. */
  NextToken?: string | null;
  /** Sort to use for grouping faces in the response. */
  SortBy?: FaceSearchSortBy | null;
}

// refs: 1 - tags: named, input
export interface GetLabelDetectionRequest {
  /** Job identifier for the label detection operation for which you want results returned. */
  JobId: string;
  /** Maximum number of results to return per paginated call. */
  MaxResults?: number | null;
  /** If the previous response was incomplete (because there are more labels to retrieve), Amazon Rekognition Video returns a pagination token in the response. */
  NextToken?: string | null;
  /** Sort to use for elements in the `Labels` array. */
  SortBy?: LabelDetectionSortBy | null;
}

// refs: 1 - tags: named, input
export interface GetPersonTrackingRequest {
  /** The identifier for a job that tracks persons in a video. */
  JobId: string;
  /** Maximum number of results to return per paginated call. */
  MaxResults?: number | null;
  /** If the previous response was incomplete (because there are more persons to retrieve), Amazon Rekognition Video returns a pagination token in the response. */
  NextToken?: string | null;
  /** Sort to use for elements in the `Persons` array. */
  SortBy?: PersonTrackingSortBy | null;
}

// refs: 1 - tags: named, input
export interface GetSegmentDetectionRequest {
  /** Job identifier for the text detection operation for which you want results returned. */
  JobId: string;
  /** Maximum number of results to return per paginated call. */
  MaxResults?: number | null;
  /** If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of text. */
  NextToken?: string | null;
}

// refs: 1 - tags: named, input
export interface GetTextDetectionRequest {
  /** Job identifier for the text detection operation for which you want results returned. */
  JobId: string;
  /** Maximum number of results to return per paginated call. */
  MaxResults?: number | null;
  /** If the previous response was incomplete (because there are more labels to retrieve), Amazon Rekognition Video returns a pagination token in the response. */
  NextToken?: string | null;
}

// refs: 1 - tags: named, input
export interface IndexFacesRequest {
  /** The ID of an existing collection to which you want to add the faces that are detected in the input images. */
  CollectionId: string;
  /** The input image as base64-encoded bytes or an S3 object. */
  Image: Image;
  /** The ID you want to assign to all the faces detected in the image. */
  ExternalImageId?: string | null;
  /** An array of facial attributes that you want to be returned. */
  DetectionAttributes?: Attribute[] | null;
  /** The maximum number of faces to index. */
  MaxFaces?: number | null;
  /** A filter that specifies a quality bar for how much filtering is done to identify faces. */
  QualityFilter?: QualityFilter | null;
}

// refs: 1 - tags: named, input
export interface ListCollectionsRequest {
  /** Pagination token from the previous response. */
  NextToken?: string | null;
  /** Maximum number of collection IDs to return. */
  MaxResults?: number | null;
}

// refs: 1 - tags: named, input
export interface ListDatasetEntriesRequest {
  /** The Amazon Resource Name (ARN) for the dataset that you want to use. */
  DatasetArn: string;
  /** Specifies a label filter for the response. */
  ContainsLabels?: string[] | null;
  /** Specify `true` to get only the JSON Lines where the image is labeled. */
  Labeled?: boolean | null;
  /** If specified, `ListDatasetEntries` only returns JSON Lines where the value of `SourceRefContains` is part of the `source-ref` field. */
  SourceRefContains?: string | null;
  /** Specifies an error filter for the response. */
  HasErrors?: boolean | null;
  /** If the previous response was incomplete (because there is more results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response. */
  NextToken?: string | null;
  /** The maximum number of results to return per paginated call. */
  MaxResults?: number | null;
}

// refs: 1 - tags: named, input
export interface ListDatasetLabelsRequest {
  /** The Amazon Resource Name (ARN) of the dataset that you want to use. */
  DatasetArn: string;
  /** If the previous response was incomplete (because there is more results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response. */
  NextToken?: string | null;
  /** The maximum number of results to return per paginated call. */
  MaxResults?: number | null;
}

// refs: 1 - tags: named, input
export interface ListFacesRequest {
  /** ID of the collection from which to list the faces. */
  CollectionId: string;
  /** If the previous response was incomplete (because there is more data to retrieve), Amazon Rekognition returns a pagination token in the response. */
  NextToken?: string | null;
  /** Maximum number of faces to return. */
  MaxResults?: number | null;
}

// refs: 1 - tags: named, input
export interface ListStreamProcessorsRequest {
  /** If the previous response was incomplete (because there are more stream processors to retrieve), Amazon Rekognition Video returns a pagination token in the response. */
  NextToken?: string | null;
  /** Maximum number of stream processors you want Amazon Rekognition Video to return in the response. */
  MaxResults?: number | null;
}

// refs: 1 - tags: named, input
export interface ListTagsForResourceRequest {
  /** Amazon Resource Name (ARN) of the model, collection, or stream processor that contains the tags that you want a list of. */
  ResourceArn: string;
}

// refs: 1 - tags: named, input
export interface RecognizeCelebritiesRequest {
  /** The input image as base64-encoded bytes or an S3 object. */
  Image: Image;
}

// refs: 1 - tags: named, input
export interface SearchFacesRequest {
  /** ID of the collection the face belongs to. */
  CollectionId: string;
  /** ID of a face to find matches for in the collection. */
  FaceId: string;
  /** Maximum number of faces to return. */
  MaxFaces?: number | null;
  /** Optional value specifying the minimum confidence in the face match to return. */
  FaceMatchThreshold?: number | null;
}

// refs: 1 - tags: named, input
export interface SearchFacesByImageRequest {
  /** ID of the collection to search. */
  CollectionId: string;
  /** The input image as base64-encoded bytes or an S3 object. */
  Image: Image;
  /** Maximum number of faces to return. */
  MaxFaces?: number | null;
  /** (Optional) Specifies the minimum confidence in the face match to return. */
  FaceMatchThreshold?: number | null;
  /** A filter that specifies a quality bar for how much filtering is done to identify faces. */
  QualityFilter?: QualityFilter | null;
}

// refs: 1 - tags: named, input
export interface StartCelebrityRecognitionRequest {
  /** The video in which you want to recognize celebrities. */
  Video: Video;
  /** Idempotent token used to identify the start request. */
  ClientRequestToken?: string | null;
  /** The Amazon SNS topic ARN that you want Amazon Rekognition Video to publish the completion status of the celebrity recognition analysis to. */
  NotificationChannel?: NotificationChannel | null;
  /** An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic. */
  JobTag?: string | null;
}

// refs: 1 - tags: named, input
export interface StartContentModerationRequest {
  /** The video in which you want to detect inappropriate, unwanted, or offensive content. */
  Video: Video;
  /** Specifies the minimum confidence that Amazon Rekognition must have in order to return a moderated content label. */
  MinConfidence?: number | null;
  /** Idempotent token used to identify the start request. */
  ClientRequestToken?: string | null;
  /** The Amazon SNS topic ARN that you want Amazon Rekognition Video to publish the completion status of the content analysis to. */
  NotificationChannel?: NotificationChannel | null;
  /** An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic. */
  JobTag?: string | null;
}

// refs: 1 - tags: named, input
export interface StartFaceDetectionRequest {
  /** The video in which you want to detect faces. */
  Video: Video;
  /** Idempotent token used to identify the start request. */
  ClientRequestToken?: string | null;
  /** The ARN of the Amazon SNS topic to which you want Amazon Rekognition Video to publish the completion status of the face detection operation. */
  NotificationChannel?: NotificationChannel | null;
  /** The face attributes you want returned. */
  FaceAttributes?: FaceAttributes | null;
  /** An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic. */
  JobTag?: string | null;
}

// refs: 1 - tags: named, input
export interface StartFaceSearchRequest {
  /** The video you want to search. */
  Video: Video;
  /** Idempotent token used to identify the start request. */
  ClientRequestToken?: string | null;
  /** The minimum confidence in the person match to return. */
  FaceMatchThreshold?: number | null;
  /** ID of the collection that contains the faces you want to search for. */
  CollectionId: string;
  /** The ARN of the Amazon SNS topic to which you want Amazon Rekognition Video to publish the completion status of the search. */
  NotificationChannel?: NotificationChannel | null;
  /** An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic. */
  JobTag?: string | null;
}

// refs: 1 - tags: named, input
export interface StartLabelDetectionRequest {
  /** The video in which you want to detect labels. */
  Video: Video;
  /** Idempotent token used to identify the start request. */
  ClientRequestToken?: string | null;
  /** Specifies the minimum confidence that Amazon Rekognition Video must have in order to return a detected label. */
  MinConfidence?: number | null;
  /** The Amazon SNS topic ARN you want Amazon Rekognition Video to publish the completion status of the label detection operation to. */
  NotificationChannel?: NotificationChannel | null;
  /** An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic. */
  JobTag?: string | null;
}

// refs: 1 - tags: named, input
export interface StartPersonTrackingRequest {
  /** The video in which you want to detect people. */
  Video: Video;
  /** Idempotent token used to identify the start request. */
  ClientRequestToken?: string | null;
  /** The Amazon SNS topic ARN you want Amazon Rekognition Video to publish the completion status of the people detection operation to. */
  NotificationChannel?: NotificationChannel | null;
  /** An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic. */
  JobTag?: string | null;
}

// refs: 1 - tags: named, input
export interface StartProjectVersionRequest {
  /** The Amazon Resource Name(ARN) of the model version that you want to start. */
  ProjectVersionArn: string;
  /** The minimum number of inference units to use. */
  MinInferenceUnits: number;
}

// refs: 1 - tags: named, input
export interface StartSegmentDetectionRequest {
  Video: Video;
  /** Idempotent token used to identify the start request. */
  ClientRequestToken?: string | null;
  /** The ARN of the Amazon SNS topic to which you want Amazon Rekognition Video to publish the completion status of the segment detection operation. */
  NotificationChannel?: NotificationChannel | null;
  /** An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic. */
  JobTag?: string | null;
  /** Filters for technical cue or shot detection. */
  Filters?: StartSegmentDetectionFilters | null;
  /** An array of segment types to detect in the video. */
  SegmentTypes: SegmentType[];
}

// refs: 1 - tags: named, input
export interface StartStreamProcessorRequest {
  /** The name of the stream processor to start processing. */
  Name: string;
}

// refs: 1 - tags: named, input
export interface StartTextDetectionRequest {
  Video: Video;
  /** Idempotent token used to identify the start request. */
  ClientRequestToken?: string | null;
  NotificationChannel?: NotificationChannel | null;
  /** An identifier returned in the completion status published by your Amazon Simple Notification Service topic. */
  JobTag?: string | null;
  /** Optional parameters that let you set criteria the text must meet to be included in your response. */
  Filters?: StartTextDetectionFilters | null;
}

// refs: 1 - tags: named, input
export interface StopProjectVersionRequest {
  /** The Amazon Resource Name (ARN) of the model version that you want to delete. */
  ProjectVersionArn: string;
}

// refs: 1 - tags: named, input
export interface StopStreamProcessorRequest {
  /** The name of a stream processor created by "CreateStreamProcessor". */
  Name: string;
}

// refs: 1 - tags: named, input
export interface TagResourceRequest {
  /** Amazon Resource Name (ARN) of the model, collection, or stream processor that you want to assign the tags to. */
  ResourceArn: string;
  /** The key-value tags to assign to the resource. */
  Tags: { [key: string]: string | null | undefined };
}

// refs: 1 - tags: named, input
export interface UntagResourceRequest {
  /** Amazon Resource Name (ARN) of the model, collection, or stream processor that you want to remove the tags from. */
  ResourceArn: string;
  /** A list of the tags that you want to remove. */
  TagKeys: string[];
}

// refs: 1 - tags: named, input
export interface UpdateDatasetEntriesRequest {
  /** The Amazon Resource Name (ARN) of the dataset that you want to update. */
  DatasetArn: string;
  /** The changes that you want to make to the dataset. */
  Changes: DatasetChanges;
}

// refs: 1 - tags: named, output
export interface CompareFacesResponse {
  /** The face in the source image that was used for comparison. */
  SourceImageFace?: ComparedSourceImageFace | null;
  /** An array of faces in the target image that match the source image face. */
  FaceMatches?: CompareFacesMatch[] | null;
  /** An array of faces in the target image that did not match the source image face. */
  UnmatchedFaces?: ComparedFace[] | null;
  /** The value of `SourceImageOrientationCorrection` is always null. */
  SourceImageOrientationCorrection?: OrientationCorrection | null;
  /** The value of `TargetImageOrientationCorrection` is always null. */
  TargetImageOrientationCorrection?: OrientationCorrection | null;
}

// refs: 1 - tags: named, output
export interface CreateCollectionResponse {
  /** HTTP status code indicating the result of the operation. */
  StatusCode?: number | null;
  /** Amazon Resource Name (ARN) of the collection. */
  CollectionArn?: string | null;
  /** Latest face model being used with the collection. */
  FaceModelVersion?: string | null;
}

// refs: 1 - tags: named, output
export interface CreateDatasetResponse {
  /** The ARN of the created Amazon Rekognition Custom Labels dataset. */
  DatasetArn?: string | null;
}

// refs: 1 - tags: named, output
export interface CreateProjectResponse {
  /** The Amazon Resource Name (ARN) of the new project. */
  ProjectArn?: string | null;
}

// refs: 1 - tags: named, output
export interface CreateProjectVersionResponse {
  /** The ARN of the model version that was created. */
  ProjectVersionArn?: string | null;
}

// refs: 1 - tags: named, output
export interface CreateStreamProcessorResponse {
  /** ARN for the newly create stream processor. */
  StreamProcessorArn?: string | null;
}

// refs: 1 - tags: named, output
export interface DeleteCollectionResponse {
  /** HTTP status code that indicates the result of the operation. */
  StatusCode?: number | null;
}

// refs: 1 - tags: named, output
export interface DeleteFacesResponse {
  /** An array of strings (face IDs) of the faces that were deleted. */
  DeletedFaces?: string[] | null;
}

// refs: 1 - tags: named, output
export interface DeleteProjectResponse {
  /** The current status of the delete project operation. */
  Status?: ProjectStatus | null;
}

// refs: 1 - tags: named, output
export interface DeleteProjectVersionResponse {
  /** The status of the deletion operation. */
  Status?: ProjectVersionStatus | null;
}

// refs: 1 - tags: named, output
export interface DescribeCollectionResponse {
  /** The number of faces that are indexed into the collection. */
  FaceCount?: number | null;
  /** The version of the face model that's used by the collection for face detection. */
  FaceModelVersion?: string | null;
  /** The Amazon Resource Name (ARN) of the collection. */
  CollectionARN?: string | null;
  /** The number of milliseconds since the Unix epoch time until the creation of the collection. */
  CreationTimestamp?: Date | number | null;
}

// refs: 1 - tags: named, output
export interface DescribeDatasetResponse {
  /** The description for the dataset. */
  DatasetDescription?: DatasetDescription | null;
}

// refs: 1 - tags: named, output
export interface DescribeProjectVersionsResponse {
  /** A list of model descriptions. */
  ProjectVersionDescriptions?: ProjectVersionDescription[] | null;
  /** If the previous response was incomplete (because there is more results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response. */
  NextToken?: string | null;
}

// refs: 1 - tags: named, output
export interface DescribeProjectsResponse {
  /** A list of project descriptions. */
  ProjectDescriptions?: ProjectDescription[] | null;
  /** If the previous response was incomplete (because there is more results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response. */
  NextToken?: string | null;
}

// refs: 1 - tags: named, output
export interface DescribeStreamProcessorResponse {
  /** Name of the stream processor. */
  Name?: string | null;
  /** ARN of the stream processor. */
  StreamProcessorArn?: string | null;
  /** Current status of the stream processor. */
  Status?: StreamProcessorStatus | null;
  /** Detailed status message about the stream processor. */
  StatusMessage?: string | null;
  /** Date and time the stream processor was created */
  CreationTimestamp?: Date | number | null;
  /** The time, in Unix format, the stream processor was last updated. */
  LastUpdateTimestamp?: Date | number | null;
  /** Kinesis video stream that provides the source streaming video. */
  Input?: StreamProcessorInput | null;
  /** Kinesis data stream to which Amazon Rekognition Video puts the analysis results. */
  Output?: StreamProcessorOutput | null;
  /** ARN of the IAM role that allows access to the stream processor. */
  RoleArn?: string | null;
  /** Face recognition input parameters that are being used by the stream processor. */
  Settings?: StreamProcessorSettings | null;
}

// refs: 1 - tags: named, output
export interface DetectCustomLabelsResponse {
  /** An array of custom labels detected in the input image. */
  CustomLabels?: CustomLabel[] | null;
}

// refs: 1 - tags: named, output
export interface DetectFacesResponse {
  /** Details of each face found in the image. */
  FaceDetails?: FaceDetail[] | null;
  /** The value of `OrientationCorrection` is always null. */
  OrientationCorrection?: OrientationCorrection | null;
}

// refs: 1 - tags: named, output
export interface DetectLabelsResponse {
  /** An array of labels for the real-world objects detected. */
  Labels?: Label[] | null;
  /** The value of `OrientationCorrection` is always null. */
  OrientationCorrection?: OrientationCorrection | null;
  /** Version number of the label detection model that was used to detect labels. */
  LabelModelVersion?: string | null;
}

// refs: 1 - tags: named, output
export interface DetectModerationLabelsResponse {
  /** Array of detected Moderation labels and the time, in milliseconds from the start of the video, they were detected. */
  ModerationLabels?: ModerationLabel[] | null;
  /** Version number of the moderation detection model that was used to detect unsafe content. */
  ModerationModelVersion?: string | null;
  /** Shows the results of the human in the loop evaluation. */
  HumanLoopActivationOutput?: HumanLoopActivationOutput | null;
}

// refs: 1 - tags: named, output
export interface DetectProtectiveEquipmentResponse {
  /** The version number of the PPE detection model used to detect PPE in the image. */
  ProtectiveEquipmentModelVersion?: string | null;
  /** An array of persons detected in the image (including persons not wearing PPE). */
  Persons?: ProtectiveEquipmentPerson[] | null;
  /** Summary information for the types of PPE specified in the `SummarizationAttributes` input parameter. */
  Summary?: ProtectiveEquipmentSummary | null;
}

// refs: 1 - tags: named, output
export interface DetectTextResponse {
  /** An array of text that was detected in the input image. */
  TextDetections?: TextDetection[] | null;
  /** The model version used to detect text. */
  TextModelVersion?: string | null;
}

// refs: 1 - tags: named, output
export interface GetCelebrityInfoResponse {
  /** An array of URLs pointing to additional celebrity information. */
  Urls?: string[] | null;
  /** The name of the celebrity. */
  Name?: string | null;
  /** Retrieves the known gender for the celebrity. */
  KnownGender?: KnownGender | null;
}

// refs: 1 - tags: named, output
export interface GetCelebrityRecognitionResponse {
  /** The current status of the celebrity recognition job. */
  JobStatus?: VideoJobStatus | null;
  /** If the job fails, `StatusMessage` provides a descriptive error message. */
  StatusMessage?: string | null;
  /** Information about a video that Amazon Rekognition Video analyzed. */
  VideoMetadata?: VideoMetadata | null;
  /** If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of celebrities. */
  NextToken?: string | null;
  /** Array of celebrities recognized in the video. */
  Celebrities?: CelebrityRecognition[] | null;
}

// refs: 1 - tags: named, output
export interface GetContentModerationResponse {
  /** The current status of the content moderation analysis job. */
  JobStatus?: VideoJobStatus | null;
  /** If the job fails, `StatusMessage` provides a descriptive error message. */
  StatusMessage?: string | null;
  /** Information about a video that Amazon Rekognition analyzed. */
  VideoMetadata?: VideoMetadata | null;
  /** The detected inappropriate, unwanted, or offensive content moderation labels and the time(s) they were detected. */
  ModerationLabels?: ContentModerationDetection[] | null;
  /** If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of content moderation labels. */
  NextToken?: string | null;
  /** Version number of the moderation detection model that was used to detect inappropriate, unwanted, or offensive content. */
  ModerationModelVersion?: string | null;
}

// refs: 1 - tags: named, output
export interface GetFaceDetectionResponse {
  /** The current status of the face detection job. */
  JobStatus?: VideoJobStatus | null;
  /** If the job fails, `StatusMessage` provides a descriptive error message. */
  StatusMessage?: string | null;
  /** Information about a video that Amazon Rekognition Video analyzed. */
  VideoMetadata?: VideoMetadata | null;
  /** If the response is truncated, Amazon Rekognition returns this token that you can use in the subsequent request to retrieve the next set of faces. */
  NextToken?: string | null;
  /** An array of faces detected in the video. */
  Faces?: FaceDetection[] | null;
}

// refs: 1 - tags: named, output
export interface GetFaceSearchResponse {
  /** The current status of the face search job. */
  JobStatus?: VideoJobStatus | null;
  /** If the job fails, `StatusMessage` provides a descriptive error message. */
  StatusMessage?: string | null;
  /** If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of search results. */
  NextToken?: string | null;
  /** Information about a video that Amazon Rekognition analyzed. */
  VideoMetadata?: VideoMetadata | null;
  /** An array of persons, "PersonMatch", in the video whose face(s) match the face(s) in an Amazon Rekognition collection. */
  Persons?: PersonMatch[] | null;
}

// refs: 1 - tags: named, output
export interface GetLabelDetectionResponse {
  /** The current status of the label detection job. */
  JobStatus?: VideoJobStatus | null;
  /** If the job fails, `StatusMessage` provides a descriptive error message. */
  StatusMessage?: string | null;
  /** Information about a video that Amazon Rekognition Video analyzed. */
  VideoMetadata?: VideoMetadata | null;
  /** If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of labels. */
  NextToken?: string | null;
  /** An array of labels detected in the video. */
  Labels?: LabelDetection[] | null;
  /** Version number of the label detection model that was used to detect labels. */
  LabelModelVersion?: string | null;
}

// refs: 1 - tags: named, output
export interface GetPersonTrackingResponse {
  /** The current status of the person tracking job. */
  JobStatus?: VideoJobStatus | null;
  /** If the job fails, `StatusMessage` provides a descriptive error message. */
  StatusMessage?: string | null;
  /** Information about a video that Amazon Rekognition Video analyzed. */
  VideoMetadata?: VideoMetadata | null;
  /** If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of persons. */
  NextToken?: string | null;
  /** An array of the persons detected in the video and the time(s) their path was tracked throughout the video. */
  Persons?: PersonDetection[] | null;
}

// refs: 1 - tags: named, output
export interface GetSegmentDetectionResponse {
  /** Current status of the segment detection job. */
  JobStatus?: VideoJobStatus | null;
  /** If the job fails, `StatusMessage` provides a descriptive error message. */
  StatusMessage?: string | null;
  /** Currently, Amazon Rekognition Video returns a single object in the `VideoMetadata` array. */
  VideoMetadata?: VideoMetadata[] | null;
  /** An array of objects. */
  AudioMetadata?: AudioMetadata[] | null;
  /** If the previous response was incomplete (because there are more labels to retrieve), Amazon Rekognition Video returns a pagination token in the response. */
  NextToken?: string | null;
  /** An array of segments detected in a video. */
  Segments?: SegmentDetection[] | null;
  /** An array containing the segment types requested in the call to `StartSegmentDetection`. */
  SelectedSegmentTypes?: SegmentTypeInfo[] | null;
}

// refs: 1 - tags: named, output
export interface GetTextDetectionResponse {
  /** Current status of the text detection job. */
  JobStatus?: VideoJobStatus | null;
  /** If the job fails, `StatusMessage` provides a descriptive error message. */
  StatusMessage?: string | null;
  VideoMetadata?: VideoMetadata | null;
  /** An array of text detected in the video. */
  TextDetections?: TextDetectionResult[] | null;
  /** If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of text. */
  NextToken?: string | null;
  /** Version number of the text detection model that was used to detect text. */
  TextModelVersion?: string | null;
}

// refs: 1 - tags: named, output
export interface IndexFacesResponse {
  /** An array of faces detected and added to the collection. */
  FaceRecords?: FaceRecord[] | null;
  /** If your collection is associated with a face detection model that's later than version 3.0, the value of `OrientationCorrection` is always null and no orientation information is returned. */
  OrientationCorrection?: OrientationCorrection | null;
  /** Latest face model being used with the collection. */
  FaceModelVersion?: string | null;
  /** An array of faces that were detected in the image but weren't indexed. */
  UnindexedFaces?: UnindexedFace[] | null;
}

// refs: 1 - tags: named, output
export interface ListCollectionsResponse {
  /** An array of collection IDs. */
  CollectionIds?: string[] | null;
  /** If the result is truncated, the response provides a `NextToken` that you can use in the subsequent request to fetch the next set of collection IDs. */
  NextToken?: string | null;
  /** Latest face models being used with the corresponding collections in the array. */
  FaceModelVersions?: string[] | null;
}

// refs: 1 - tags: named, output
export interface ListDatasetEntriesResponse {
  /** A list of entries (images) in the dataset. */
  DatasetEntries?: string[] | null;
  /** If the previous response was incomplete (because there is more results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response. */
  NextToken?: string | null;
}

// refs: 1 - tags: named, output
export interface ListDatasetLabelsResponse {
  /** A list of the labels in the dataset. */
  DatasetLabelDescriptions?: DatasetLabelDescription[] | null;
  /** If the previous response was incomplete (because there is more results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response. */
  NextToken?: string | null;
}

// refs: 1 - tags: named, output
export interface ListFacesResponse {
  /** An array of `Face` objects. */
  Faces?: Face[] | null;
  /** If the response is truncated, Amazon Rekognition returns this token that you can use in the subsequent request to retrieve the next set of faces. */
  NextToken?: string | null;
  /** Latest face model being used with the collection. */
  FaceModelVersion?: string | null;
}

// refs: 1 - tags: named, output
export interface ListStreamProcessorsResponse {
  /** If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of stream processors. */
  NextToken?: string | null;
  /** List of stream processors that you have created. */
  StreamProcessors?: StreamProcessor[] | null;
}

// refs: 1 - tags: named, output
export interface ListTagsForResourceResponse {
  /** A list of key-value tags assigned to the resource. */
  Tags?: { [key: string]: string | null | undefined } | null;
}

// refs: 1 - tags: named, output
export interface RecognizeCelebritiesResponse {
  /** Details about each celebrity found in the image. */
  CelebrityFaces?: Celebrity[] | null;
  /** Details about each unrecognized face in the image. */
  UnrecognizedFaces?: ComparedFace[] | null;
  /**   Note: */
  OrientationCorrection?: OrientationCorrection | null;
}

// refs: 1 - tags: named, output
export interface SearchFacesResponse {
  /** ID of the face that was searched for matches in a collection. */
  SearchedFaceId?: string | null;
  /** An array of faces that matched the input face, along with the confidence in the match. */
  FaceMatches?: FaceMatch[] | null;
  /** Latest face model being used with the collection. */
  FaceModelVersion?: string | null;
}

// refs: 1 - tags: named, output
export interface SearchFacesByImageResponse {
  /** The bounding box around the face in the input image that Amazon Rekognition used for the search. */
  SearchedFaceBoundingBox?: BoundingBox | null;
  /** The level of confidence that the `searchedFaceBoundingBox`, contains a face. */
  SearchedFaceConfidence?: number | null;
  /** An array of faces that match the input face, along with the confidence in the match. */
  FaceMatches?: FaceMatch[] | null;
  /** Latest face model being used with the collection. */
  FaceModelVersion?: string | null;
}

// refs: 1 - tags: named, output
export interface StartCelebrityRecognitionResponse {
  /** The identifier for the celebrity recognition analysis job. */
  JobId?: string | null;
}

// refs: 1 - tags: named, output
export interface StartContentModerationResponse {
  /** The identifier for the content analysis job. */
  JobId?: string | null;
}

// refs: 1 - tags: named, output
export interface StartFaceDetectionResponse {
  /** The identifier for the face detection job. */
  JobId?: string | null;
}

// refs: 1 - tags: named, output
export interface StartFaceSearchResponse {
  /** The identifier for the search job. */
  JobId?: string | null;
}

// refs: 1 - tags: named, output
export interface StartLabelDetectionResponse {
  /** The identifier for the label detection job. */
  JobId?: string | null;
}

// refs: 1 - tags: named, output
export interface StartPersonTrackingResponse {
  /** The identifier for the person detection job. */
  JobId?: string | null;
}

// refs: 1 - tags: named, output
export interface StartProjectVersionResponse {
  /** The current running status of the model. */
  Status?: ProjectVersionStatus | null;
}

// refs: 1 - tags: named, output
export interface StartSegmentDetectionResponse {
  /** Unique identifier for the segment detection job. */
  JobId?: string | null;
}

// refs: 1 - tags: named, output
export interface StartTextDetectionResponse {
  /** Identifier for the text detection job. */
  JobId?: string | null;
}

// refs: 1 - tags: named, output
export interface StopProjectVersionResponse {
  /** The current status of the stop operation. */
  Status?: ProjectVersionStatus | null;
}

// refs: 11 - tags: input, named, interface
/** Provides the input image either as bytes or an S3 object. */
export interface Image {
  /** Blob of image bytes up to 5 MBs. */
  Bytes?: Uint8Array | string | null;
  /** Identifies an S3 object as the image source. */
  S3Object?: S3Object | null;
}
export function fromImage(input?: Image | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    Bytes: serializeBlob(input["Bytes"]),
    S3Object: fromS3Object(input["S3Object"]),
  }
}

// refs: 30 - tags: input, named, interface, output
/** Provides the S3 bucket name and object name. */
export interface S3Object {
  /** Name of the S3 bucket. */
  Bucket?: string | null;
  /** S3 object key name. */
  Name?: string | null;
  /** If the bucket is versioning enabled, you can specify the object version. */
  Version?: string | null;
}
export function fromS3Object(input?: S3Object | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    Bucket: input["Bucket"],
    Name: input["Name"],
    Version: input["Version"],
  }
}
export function toS3Object(root: jsonP.JSONValue): S3Object {
  return jsonP.readObj({
    required: {},
    optional: {
      "Bucket": "s",
      "Name": "s",
      "Version": "s",
    },
  }, root);
}

// refs: 3 - tags: input, named, enum
export type QualityFilter =
| "NONE"
| "AUTO"
| "LOW"
| "MEDIUM"
| "HIGH"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: input, named, interface
/** The source that Amazon Rekognition Custom Labels uses to create a dataset. */
export interface DatasetSource {
  GroundTruthManifest?: GroundTruthManifest | null;
  /** The ARN of an Amazon Rekognition Custom Labels dataset that you want to copy. */
  DatasetArn?: string | null;
}
export function fromDatasetSource(input?: DatasetSource | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    GroundTruthManifest: fromGroundTruthManifest(input["GroundTruthManifest"]),
    DatasetArn: input["DatasetArn"],
  }
}

// refs: 10 - tags: input, named, interface, output
/** The S3 bucket that contains an Amazon Sagemaker Ground Truth format manifest file. */
export interface GroundTruthManifest {
  S3Object?: S3Object | null;
}
export function fromGroundTruthManifest(input?: GroundTruthManifest | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    S3Object: fromS3Object(input["S3Object"]),
  }
}
export function toGroundTruthManifest(root: jsonP.JSONValue): GroundTruthManifest {
  return jsonP.readObj({
    required: {},
    optional: {
      "S3Object": toS3Object,
    },
  }, root);
}

// refs: 2 - tags: input, named, enum, output
export type DatasetType =
| "TRAIN"
| "TEST"
| cmnP.UnexpectedEnumValue;

// refs: 2 - tags: input, named, interface, output
/** The S3 bucket and folder location where training output is placed. */
export interface OutputConfig {
  /** The S3 bucket where training output is placed. */
  S3Bucket?: string | null;
  /** The prefix applied to the training output files. */
  S3KeyPrefix?: string | null;
}
export function fromOutputConfig(input?: OutputConfig | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    S3Bucket: input["S3Bucket"],
    S3KeyPrefix: input["S3KeyPrefix"],
  }
}
export function toOutputConfig(root: jsonP.JSONValue): OutputConfig {
  return jsonP.readObj({
    required: {},
    optional: {
      "S3Bucket": "s",
      "S3KeyPrefix": "s",
    },
  }, root);
}

// refs: 3 - tags: input, named, interface, output
/** The dataset used for training. */
export interface TrainingData {
  /** A Sagemaker GroundTruth manifest file that contains the training images (assets). */
  Assets?: Asset[] | null;
}
export function fromTrainingData(input?: TrainingData | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    Assets: input["Assets"]?.map(x => fromAsset(x)),
  }
}
export function toTrainingData(root: jsonP.JSONValue): TrainingData {
  return jsonP.readObj({
    required: {},
    optional: {
      "Assets": [toAsset],
    },
  }, root);
}

// refs: 8 - tags: input, named, interface, output
/** Assets are the images that you use to train and evaluate a model version. */
export interface Asset {
  GroundTruthManifest?: GroundTruthManifest | null;
}
export function fromAsset(input?: Asset | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    GroundTruthManifest: fromGroundTruthManifest(input["GroundTruthManifest"]),
  }
}
export function toAsset(root: jsonP.JSONValue): Asset {
  return jsonP.readObj({
    required: {},
    optional: {
      "GroundTruthManifest": toGroundTruthManifest,
    },
  }, root);
}

// refs: 3 - tags: input, named, interface, output
/** The dataset used for testing. */
export interface TestingData {
  /** The assets used for testing. */
  Assets?: Asset[] | null;
  /** If specified, Amazon Rekognition Custom Labels temporarily splits the training dataset (80%) to create a test dataset (20%) for the training job. */
  AutoCreate?: boolean | null;
}
export function fromTestingData(input?: TestingData | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    Assets: input["Assets"]?.map(x => fromAsset(x)),
    AutoCreate: input["AutoCreate"],
  }
}
export function toTestingData(root: jsonP.JSONValue): TestingData {
  return jsonP.readObj({
    required: {},
    optional: {
      "Assets": [toAsset],
      "AutoCreate": "b",
    },
  }, root);
}

// refs: 2 - tags: input, named, interface, output
/** Information about the source streaming video. */
export interface StreamProcessorInput {
  /** The Kinesis video stream input stream for the source streaming video. */
  KinesisVideoStream?: KinesisVideoStream | null;
}
export function fromStreamProcessorInput(input?: StreamProcessorInput | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    KinesisVideoStream: fromKinesisVideoStream(input["KinesisVideoStream"]),
  }
}
export function toStreamProcessorInput(root: jsonP.JSONValue): StreamProcessorInput {
  return jsonP.readObj({
    required: {},
    optional: {
      "KinesisVideoStream": toKinesisVideoStream,
    },
  }, root);
}

// refs: 2 - tags: input, named, interface, output
/** Kinesis video stream stream that provides the source streaming video for a Amazon Rekognition Video stream processor. */
export interface KinesisVideoStream {
  /** ARN of the Kinesis video stream stream that streams the source video. */
  Arn?: string | null;
}
export function fromKinesisVideoStream(input?: KinesisVideoStream | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    Arn: input["Arn"],
  }
}
export function toKinesisVideoStream(root: jsonP.JSONValue): KinesisVideoStream {
  return jsonP.readObj({
    required: {},
    optional: {
      "Arn": "s",
    },
  }, root);
}

// refs: 2 - tags: input, named, interface, output
/** Information about the Amazon Kinesis Data Streams stream to which a Amazon Rekognition Video stream processor streams the results of a video analysis. */
export interface StreamProcessorOutput {
  /** The Amazon Kinesis Data Streams stream to which the Amazon Rekognition stream processor streams the analysis results. */
  KinesisDataStream?: KinesisDataStream | null;
}
export function fromStreamProcessorOutput(input?: StreamProcessorOutput | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    KinesisDataStream: fromKinesisDataStream(input["KinesisDataStream"]),
  }
}
export function toStreamProcessorOutput(root: jsonP.JSONValue): StreamProcessorOutput {
  return jsonP.readObj({
    required: {},
    optional: {
      "KinesisDataStream": toKinesisDataStream,
    },
  }, root);
}

// refs: 2 - tags: input, named, interface, output
/** The Kinesis data stream Amazon Rekognition to which the analysis results of a Amazon Rekognition stream processor are streamed. */
export interface KinesisDataStream {
  /** ARN of the output Amazon Kinesis Data Streams stream. */
  Arn?: string | null;
}
export function fromKinesisDataStream(input?: KinesisDataStream | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    Arn: input["Arn"],
  }
}
export function toKinesisDataStream(root: jsonP.JSONValue): KinesisDataStream {
  return jsonP.readObj({
    required: {},
    optional: {
      "Arn": "s",
    },
  }, root);
}

// refs: 2 - tags: input, named, interface, output
/** Input parameters used to recognize faces in a streaming video analyzed by a Amazon Rekognition stream processor. */
export interface StreamProcessorSettings {
  /** Face search settings to use on a streaming video. */
  FaceSearch?: FaceSearchSettings | null;
}
export function fromStreamProcessorSettings(input?: StreamProcessorSettings | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    FaceSearch: fromFaceSearchSettings(input["FaceSearch"]),
  }
}
export function toStreamProcessorSettings(root: jsonP.JSONValue): StreamProcessorSettings {
  return jsonP.readObj({
    required: {},
    optional: {
      "FaceSearch": toFaceSearchSettings,
    },
  }, root);
}

// refs: 2 - tags: input, named, interface, output
/** Input face recognition parameters for an Amazon Rekognition stream processor. */
export interface FaceSearchSettings {
  /** The ID of a collection that contains faces that you want to search for. */
  CollectionId?: string | null;
  /** Minimum face match confidence score that must be met to return a result for a recognized face. */
  FaceMatchThreshold?: number | null;
}
export function fromFaceSearchSettings(input?: FaceSearchSettings | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    CollectionId: input["CollectionId"],
    FaceMatchThreshold: input["FaceMatchThreshold"],
  }
}
export function toFaceSearchSettings(root: jsonP.JSONValue): FaceSearchSettings {
  return jsonP.readObj({
    required: {},
    optional: {
      "CollectionId": "s",
      "FaceMatchThreshold": "n",
    },
  }, root);
}

// refs: 2 - tags: input, named, enum
export type Attribute =
| "DEFAULT"
| "ALL"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: input, named, interface
/** Sets up the flow definition the image will be sent to if one of the conditions is met. */
export interface HumanLoopConfig {
  /** The name of the human review used for this image. */
  HumanLoopName: string;
  /** The Amazon Resource Name (ARN) of the flow definition. */
  FlowDefinitionArn: string;
  /** Sets attributes of the input data. */
  DataAttributes?: HumanLoopDataAttributes | null;
}
export function fromHumanLoopConfig(input?: HumanLoopConfig | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    HumanLoopName: input["HumanLoopName"],
    FlowDefinitionArn: input["FlowDefinitionArn"],
    DataAttributes: fromHumanLoopDataAttributes(input["DataAttributes"]),
  }
}

// refs: 1 - tags: input, named, interface
/** Allows you to set attributes of the image. */
export interface HumanLoopDataAttributes {
  /** Sets whether the input image is free of personally identifiable information. */
  ContentClassifiers?: ContentClassifier[] | null;
}
export function fromHumanLoopDataAttributes(input?: HumanLoopDataAttributes | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    ContentClassifiers: input["ContentClassifiers"],
  }
}

// refs: 1 - tags: input, named, enum
export type ContentClassifier =
| "FreeOfPersonallyIdentifiableInformation"
| "FreeOfAdultContent"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: input, named, interface
/** Specifies summary attributes to return from a call to "DetectProtectiveEquipment". */
export interface ProtectiveEquipmentSummarizationAttributes {
  /** The minimum confidence level for which you want summary information. */
  MinConfidence: number;
  /** An array of personal protective equipment types for which you want summary information. */
  RequiredEquipmentTypes: ProtectiveEquipmentType[];
}
export function fromProtectiveEquipmentSummarizationAttributes(input?: ProtectiveEquipmentSummarizationAttributes | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    MinConfidence: input["MinConfidence"],
    RequiredEquipmentTypes: input["RequiredEquipmentTypes"],
  }
}

// refs: 2 - tags: input, named, enum, output
export type ProtectiveEquipmentType =
| "FACE_COVER"
| "HAND_COVER"
| "HEAD_COVER"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: input, named, interface
/** A set of optional parameters that you can use to set the criteria that the text must meet to be included in your response. */
export interface DetectTextFilters {
  WordFilter?: DetectionFilter | null;
  /** A Filter focusing on a certain area of the image. */
  RegionsOfInterest?: RegionOfInterest[] | null;
}
export function fromDetectTextFilters(input?: DetectTextFilters | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    WordFilter: fromDetectionFilter(input["WordFilter"]),
    RegionsOfInterest: input["RegionsOfInterest"]?.map(x => fromRegionOfInterest(x)),
  }
}

// refs: 2 - tags: input, named, interface
/** A set of parameters that allow you to filter out certain results from your returned results. */
export interface DetectionFilter {
  /** Sets the confidence of word detection. */
  MinConfidence?: number | null;
  /** Sets the minimum height of the word bounding box. */
  MinBoundingBoxHeight?: number | null;
  /** Sets the minimum width of the word bounding box. */
  MinBoundingBoxWidth?: number | null;
}
export function fromDetectionFilter(input?: DetectionFilter | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    MinConfidence: input["MinConfidence"],
    MinBoundingBoxHeight: input["MinBoundingBoxHeight"],
    MinBoundingBoxWidth: input["MinBoundingBoxWidth"],
  }
}

// refs: 2 - tags: input, named, interface
/** Specifies a location within the frame that Rekognition checks for text. */
export interface RegionOfInterest {
  /** The box representing a region of interest on screen. */
  BoundingBox?: BoundingBox | null;
}
export function fromRegionOfInterest(input?: RegionOfInterest | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    BoundingBox: fromBoundingBox(input["BoundingBox"]),
  }
}

// refs: 30 - tags: input, named, interface, output
/** Identifies the bounding box around the label, face, text or personal protective equipment. */
export interface BoundingBox {
  /** Width of the bounding box as a ratio of the overall image width. */
  Width?: number | null;
  /** Height of the bounding box as a ratio of the overall image height. */
  Height?: number | null;
  /** Left coordinate of the bounding box as a ratio of overall image width. */
  Left?: number | null;
  /** Top coordinate of the bounding box as a ratio of overall image height. */
  Top?: number | null;
}
export function fromBoundingBox(input?: BoundingBox | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    Width: input["Width"],
    Height: input["Height"],
    Left: input["Left"],
    Top: input["Top"],
  }
}
export function toBoundingBox(root: jsonP.JSONValue): BoundingBox {
  return jsonP.readObj({
    required: {},
    optional: {
      "Width": "n",
      "Height": "n",
      "Left": "n",
      "Top": "n",
    },
  }, root);
}

// refs: 1 - tags: input, named, interface
/** A training dataset or a test dataset used in a dataset distribution operation. */
export interface DistributeDataset {
  /** The Amazon Resource Name (ARN) of the dataset that you want to use. */
  Arn: string;
}
export function fromDistributeDataset(input?: DistributeDataset | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    Arn: input["Arn"],
  }
}

// refs: 1 - tags: input, named, enum
export type CelebrityRecognitionSortBy =
| "ID"
| "TIMESTAMP"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: input, named, enum
export type ContentModerationSortBy =
| "NAME"
| "TIMESTAMP"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: input, named, enum
export type FaceSearchSortBy =
| "INDEX"
| "TIMESTAMP"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: input, named, enum
export type LabelDetectionSortBy =
| "NAME"
| "TIMESTAMP"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: input, named, enum
export type PersonTrackingSortBy =
| "INDEX"
| "TIMESTAMP"
| cmnP.UnexpectedEnumValue;

// refs: 8 - tags: input, named, interface
/** Video file stored in an Amazon S3 bucket. */
export interface Video {
  /** The Amazon S3 bucket name and file name for the video. */
  S3Object?: S3Object | null;
}
export function fromVideo(input?: Video | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    S3Object: fromS3Object(input["S3Object"]),
  }
}

// refs: 8 - tags: input, named, interface
/** The Amazon Simple Notification Service topic to which Amazon Rekognition publishes the completion status of a video analysis operation. */
export interface NotificationChannel {
  /** The Amazon SNS topic to which Amazon Rekognition to posts the completion status. */
  SNSTopicArn: string;
  /** The ARN of an IAM role that gives Amazon Rekognition publishing permissions to the Amazon SNS topic. */
  RoleArn: string;
}
export function fromNotificationChannel(input?: NotificationChannel | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    SNSTopicArn: input["SNSTopicArn"],
    RoleArn: input["RoleArn"],
  }
}

// refs: 1 - tags: input, named, enum
export type FaceAttributes =
| "DEFAULT"
| "ALL"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: input, named, interface
/** Filters applied to the technical cue or shot detection segments. */
export interface StartSegmentDetectionFilters {
  /** Filters that are specific to technical cues. */
  TechnicalCueFilter?: StartTechnicalCueDetectionFilter | null;
  /** Filters that are specific to shot detections. */
  ShotFilter?: StartShotDetectionFilter | null;
}
export function fromStartSegmentDetectionFilters(input?: StartSegmentDetectionFilters | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    TechnicalCueFilter: fromStartTechnicalCueDetectionFilter(input["TechnicalCueFilter"]),
    ShotFilter: fromStartShotDetectionFilter(input["ShotFilter"]),
  }
}

// refs: 1 - tags: input, named, interface
/** Filters for the technical segments returned by "GetSegmentDetection". */
export interface StartTechnicalCueDetectionFilter {
  /** Specifies the minimum confidence that Amazon Rekognition Video must have in order to return a detected segment. */
  MinSegmentConfidence?: number | null;
  /** A filter that allows you to control the black frame detection by specifying the black levels and pixel coverage of black pixels in a frame. */
  BlackFrame?: BlackFrame | null;
}
export function fromStartTechnicalCueDetectionFilter(input?: StartTechnicalCueDetectionFilter | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    MinSegmentConfidence: input["MinSegmentConfidence"],
    BlackFrame: fromBlackFrame(input["BlackFrame"]),
  }
}

// refs: 1 - tags: input, named, interface
/** A filter that allows you to control the black frame detection by specifying the black levels and pixel coverage of black pixels in a frame. */
export interface BlackFrame {
  /** A threshold used to determine the maximum luminance value for a pixel to be considered black. */
  MaxPixelThreshold?: number | null;
  /** The minimum percentage of pixels in a frame that need to have a luminance below the max_black_pixel_value for a frame to be considered a black frame. */
  MinCoveragePercentage?: number | null;
}
export function fromBlackFrame(input?: BlackFrame | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    MaxPixelThreshold: input["MaxPixelThreshold"],
    MinCoveragePercentage: input["MinCoveragePercentage"],
  }
}

// refs: 1 - tags: input, named, interface
/** Filters for the shot detection segments returned by `GetSegmentDetection`. */
export interface StartShotDetectionFilter {
  /** Specifies the minimum confidence that Amazon Rekognition Video must have in order to return a detected segment. */
  MinSegmentConfidence?: number | null;
}
export function fromStartShotDetectionFilter(input?: StartShotDetectionFilter | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    MinSegmentConfidence: input["MinSegmentConfidence"],
  }
}

// refs: 3 - tags: input, named, enum, output
export type SegmentType =
| "TECHNICAL_CUE"
| "SHOT"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: input, named, interface
/** Set of optional parameters that let you set the criteria text must meet to be included in your response. */
export interface StartTextDetectionFilters {
  /** Filters focusing on qualities of the text, such as confidence or size. */
  WordFilter?: DetectionFilter | null;
  /** Filter focusing on a certain area of the frame. */
  RegionsOfInterest?: RegionOfInterest[] | null;
}
export function fromStartTextDetectionFilters(input?: StartTextDetectionFilters | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    WordFilter: fromDetectionFilter(input["WordFilter"]),
    RegionsOfInterest: input["RegionsOfInterest"]?.map(x => fromRegionOfInterest(x)),
  }
}

// refs: 1 - tags: input, named, interface
/** Describes updates or additions to a dataset. */
export interface DatasetChanges {
  /** A Base64-encoded binary data object containing one or JSON lines that either update the dataset or are additions to the dataset. */
  GroundTruth: Uint8Array | string;
}
export function fromDatasetChanges(input?: DatasetChanges | null): jsonP.JSONValue {
  if (!input) return input;
  return {
    GroundTruth: serializeBlob(input["GroundTruth"]),
  }
}

// refs: 1 - tags: output, named, interface
/** Type that describes the face Amazon Rekognition chose to compare with the faces in the target. */
export interface ComparedSourceImageFace {
  /** Bounding box of the face. */
  BoundingBox?: BoundingBox | null;
  /** Confidence level that the selected bounding box contains a face. */
  Confidence?: number | null;
}
export function toComparedSourceImageFace(root: jsonP.JSONValue): ComparedSourceImageFace {
  return jsonP.readObj({
    required: {},
    optional: {
      "BoundingBox": toBoundingBox,
      "Confidence": "n",
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Provides information about a face in a target image that matches the source image face analyzed by `CompareFaces`. */
export interface CompareFacesMatch {
  /** Level of confidence that the faces match. */
  Similarity?: number | null;
  /** Provides face metadata (bounding box and confidence that the bounding box actually contains a face). */
  Face?: ComparedFace | null;
}
export function toCompareFacesMatch(root: jsonP.JSONValue): CompareFacesMatch {
  return jsonP.readObj({
    required: {},
    optional: {
      "Similarity": "n",
      "Face": toComparedFace,
    },
  }, root);
}

// refs: 4 - tags: output, named, interface
/** Provides face metadata for target image faces that are analyzed by `CompareFaces` and `RecognizeCelebrities`. */
export interface ComparedFace {
  /** Bounding box of the face. */
  BoundingBox?: BoundingBox | null;
  /** Level of confidence that what the bounding box contains is a face. */
  Confidence?: number | null;
  /** An array of facial landmarks. */
  Landmarks?: Landmark[] | null;
  /** Indicates the pose of the face as determined by its pitch, roll, and yaw. */
  Pose?: Pose | null;
  /** Identifies face image brightness and sharpness. */
  Quality?: ImageQuality | null;
  /** The emotions that appear to be expressed on the face, and the confidence level in the determination. */
  Emotions?: Emotion[] | null;
  /** Indicates whether or not the face is smiling, and the confidence level in the determination. */
  Smile?: Smile | null;
}
export function toComparedFace(root: jsonP.JSONValue): ComparedFace {
  return jsonP.readObj({
    required: {},
    optional: {
      "BoundingBox": toBoundingBox,
      "Confidence": "n",
      "Landmarks": [toLandmark],
      "Pose": toPose,
      "Quality": toImageQuality,
      "Emotions": [toEmotion],
      "Smile": toSmile,
    },
  }, root);
}

// refs: 11 - tags: output, named, interface
/** Indicates the location of the landmark on the face. */
export interface Landmark {
  /** Type of landmark. */
  Type?: LandmarkType | null;
  /** The x-coordinate of the landmark expressed as a ratio of the width of the image. */
  X?: number | null;
  /** The y-coordinate of the landmark expressed as a ratio of the height of the image. */
  Y?: number | null;
}
export function toLandmark(root: jsonP.JSONValue): Landmark {
  return jsonP.readObj({
    required: {},
    optional: {
      "Type": (x: jsonP.JSONValue) => cmnP.readEnum<LandmarkType>(x),
      "X": "n",
      "Y": "n",
    },
  }, root);
}

// refs: 11 - tags: output, named, enum
export type LandmarkType =
| "eyeLeft"
| "eyeRight"
| "nose"
| "mouthLeft"
| "mouthRight"
| "leftEyeBrowLeft"
| "leftEyeBrowRight"
| "leftEyeBrowUp"
| "rightEyeBrowLeft"
| "rightEyeBrowRight"
| "rightEyeBrowUp"
| "leftEyeLeft"
| "leftEyeRight"
| "leftEyeUp"
| "leftEyeDown"
| "rightEyeLeft"
| "rightEyeRight"
| "rightEyeUp"
| "rightEyeDown"
| "noseLeft"
| "noseRight"
| "mouthUp"
| "mouthDown"
| "leftPupil"
| "rightPupil"
| "upperJawlineLeft"
| "midJawlineLeft"
| "chinBottom"
| "midJawlineRight"
| "upperJawlineRight"
| cmnP.UnexpectedEnumValue;

// refs: 11 - tags: output, named, interface
/** Indicates the pose of the face as determined by its pitch, roll, and yaw. */
export interface Pose {
  /** Value representing the face rotation on the roll axis. */
  Roll?: number | null;
  /** Value representing the face rotation on the yaw axis. */
  Yaw?: number | null;
  /** Value representing the face rotation on the pitch axis. */
  Pitch?: number | null;
}
export function toPose(root: jsonP.JSONValue): Pose {
  return jsonP.readObj({
    required: {},
    optional: {
      "Roll": "n",
      "Yaw": "n",
      "Pitch": "n",
    },
  }, root);
}

// refs: 11 - tags: output, named, interface
/** Identifies face image brightness and sharpness. */
export interface ImageQuality {
  /** Value representing brightness of the face. */
  Brightness?: number | null;
  /** Value representing sharpness of the face. */
  Sharpness?: number | null;
}
export function toImageQuality(root: jsonP.JSONValue): ImageQuality {
  return jsonP.readObj({
    required: {},
    optional: {
      "Brightness": "n",
      "Sharpness": "n",
    },
  }, root);
}

// refs: 11 - tags: output, named, interface
/** The emotions that appear to be expressed on the face, and the confidence level in the determination. */
export interface Emotion {
  /** Type of emotion detected. */
  Type?: EmotionName | null;
  /** Level of confidence in the determination. */
  Confidence?: number | null;
}
export function toEmotion(root: jsonP.JSONValue): Emotion {
  return jsonP.readObj({
    required: {},
    optional: {
      "Type": (x: jsonP.JSONValue) => cmnP.readEnum<EmotionName>(x),
      "Confidence": "n",
    },
  }, root);
}

// refs: 11 - tags: output, named, enum
export type EmotionName =
| "HAPPY"
| "SAD"
| "ANGRY"
| "CONFUSED"
| "DISGUSTED"
| "SURPRISED"
| "CALM"
| "UNKNOWN"
| "FEAR"
| cmnP.UnexpectedEnumValue;

// refs: 11 - tags: output, named, interface
/** Indicates whether or not the face is smiling, and the confidence level in the determination. */
export interface Smile {
  /** Boolean value that indicates whether the face is smiling or not. */
  Value?: boolean | null;
  /** Level of confidence in the determination. */
  Confidence?: number | null;
}
export function toSmile(root: jsonP.JSONValue): Smile {
  return jsonP.readObj({
    required: {},
    optional: {
      "Value": "b",
      "Confidence": "n",
    },
  }, root);
}

// refs: 6 - tags: output, named, enum
export type OrientationCorrection =
| "ROTATE_0"
| "ROTATE_90"
| "ROTATE_180"
| "ROTATE_270"
| cmnP.UnexpectedEnumValue;

// refs: 2 - tags: output, named, enum
export type ProjectStatus =
| "CREATING"
| "CREATED"
| "DELETING"
| cmnP.UnexpectedEnumValue;

// refs: 4 - tags: output, named, enum
export type ProjectVersionStatus =
| "TRAINING_IN_PROGRESS"
| "TRAINING_COMPLETED"
| "TRAINING_FAILED"
| "STARTING"
| "RUNNING"
| "FAILED"
| "STOPPING"
| "STOPPED"
| "DELETING"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: output, named, interface
/** A description for a dataset. */
export interface DatasetDescription {
  /** The Unix timestamp for the time and date that the dataset was created. */
  CreationTimestamp?: Date | number | null;
  /** The Unix timestamp for the date and time that the dataset was last updated. */
  LastUpdatedTimestamp?: Date | number | null;
  /** The status of the dataset. */
  Status?: DatasetStatus | null;
  /** The status message for the dataset. */
  StatusMessage?: string | null;
  /** The status message code for the dataset operation. */
  StatusMessageCode?: DatasetStatusMessageCode | null;
  /** The status message code for the dataset. */
  DatasetStats?: DatasetStats | null;
}
export function toDatasetDescription(root: jsonP.JSONValue): DatasetDescription {
  return jsonP.readObj({
    required: {},
    optional: {
      "CreationTimestamp": "d",
      "LastUpdatedTimestamp": "d",
      "Status": (x: jsonP.JSONValue) => cmnP.readEnum<DatasetStatus>(x),
      "StatusMessage": "s",
      "StatusMessageCode": (x: jsonP.JSONValue) => cmnP.readEnum<DatasetStatusMessageCode>(x),
      "DatasetStats": toDatasetStats,
    },
  }, root);
}

// refs: 2 - tags: output, named, enum
export type DatasetStatus =
| "CREATE_IN_PROGRESS"
| "CREATE_COMPLETE"
| "CREATE_FAILED"
| "UPDATE_IN_PROGRESS"
| "UPDATE_COMPLETE"
| "UPDATE_FAILED"
| "DELETE_IN_PROGRESS"
| cmnP.UnexpectedEnumValue;

// refs: 2 - tags: output, named, enum
export type DatasetStatusMessageCode =
| "SUCCESS"
| "SERVICE_ERROR"
| "CLIENT_ERROR"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: output, named, interface
/** Provides statistics about a dataset. */
export interface DatasetStats {
  /** The total number of images in the dataset that have labels. */
  LabeledEntries?: number | null;
  /** The total number of images in the dataset. */
  TotalEntries?: number | null;
  /** The total number of labels declared in the dataset. */
  TotalLabels?: number | null;
  /** The total number of entries that contain at least one error. */
  ErrorEntries?: number | null;
}
export function toDatasetStats(root: jsonP.JSONValue): DatasetStats {
  return jsonP.readObj({
    required: {},
    optional: {
      "LabeledEntries": "n",
      "TotalEntries": "n",
      "TotalLabels": "n",
      "ErrorEntries": "n",
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** A description of a version of an Amazon Rekognition Custom Labels model. */
export interface ProjectVersionDescription {
  /** The Amazon Resource Name (ARN) of the model version. */
  ProjectVersionArn?: string | null;
  /** The Unix datetime for the date and time that training started. */
  CreationTimestamp?: Date | number | null;
  /** The minimum number of inference units used by the model. */
  MinInferenceUnits?: number | null;
  /** The current status of the model version. */
  Status?: ProjectVersionStatus | null;
  /** A descriptive message for an error or warning that occurred. */
  StatusMessage?: string | null;
  /** The duration, in seconds, that you were billed for a successful training of the model version. */
  BillableTrainingTimeInSeconds?: number | null;
  /** The Unix date and time that training of the model ended. */
  TrainingEndTimestamp?: Date | number | null;
  /** The location where training results are saved. */
  OutputConfig?: OutputConfig | null;
  /** Contains information about the training results. */
  TrainingDataResult?: TrainingDataResult | null;
  /** Contains information about the testing results. */
  TestingDataResult?: TestingDataResult | null;
  /** The training results. */
  EvaluationResult?: EvaluationResult | null;
  /** The location of the summary manifest. */
  ManifestSummary?: GroundTruthManifest | null;
  /** The identifer for the AWS Key Management Service key (AWS KMS key) that was used to encrypt the model during training. */
  KmsKeyId?: string | null;
}
export function toProjectVersionDescription(root: jsonP.JSONValue): ProjectVersionDescription {
  return jsonP.readObj({
    required: {},
    optional: {
      "ProjectVersionArn": "s",
      "CreationTimestamp": "d",
      "MinInferenceUnits": "n",
      "Status": (x: jsonP.JSONValue) => cmnP.readEnum<ProjectVersionStatus>(x),
      "StatusMessage": "s",
      "BillableTrainingTimeInSeconds": "n",
      "TrainingEndTimestamp": "d",
      "OutputConfig": toOutputConfig,
      "TrainingDataResult": toTrainingDataResult,
      "TestingDataResult": toTestingDataResult,
      "EvaluationResult": toEvaluationResult,
      "ManifestSummary": toGroundTruthManifest,
      "KmsKeyId": "s",
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Sagemaker Groundtruth format manifest files for the input, output and validation datasets that are used and created during testing. */
export interface TrainingDataResult {
  /** The training assets that you supplied for training. */
  Input?: TrainingData | null;
  /** The images (assets) that were actually trained by Amazon Rekognition Custom Labels. */
  Output?: TrainingData | null;
  /** The location of the data validation manifest. */
  Validation?: ValidationData | null;
}
export function toTrainingDataResult(root: jsonP.JSONValue): TrainingDataResult {
  return jsonP.readObj({
    required: {},
    optional: {
      "Input": toTrainingData,
      "Output": toTrainingData,
      "Validation": toValidationData,
    },
  }, root);
}

// refs: 2 - tags: output, named, interface
/** Contains the Amazon S3 bucket location of the validation data for a model training job. */
export interface ValidationData {
  /** The assets that comprise the validation data. */
  Assets?: Asset[] | null;
}
export function toValidationData(root: jsonP.JSONValue): ValidationData {
  return jsonP.readObj({
    required: {},
    optional: {
      "Assets": [toAsset],
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Sagemaker Groundtruth format manifest files for the input, output and validation datasets that are used and created during testing. */
export interface TestingDataResult {
  /** The testing dataset that was supplied for training. */
  Input?: TestingData | null;
  /** The subset of the dataset that was actually tested. */
  Output?: TestingData | null;
  /** The location of the data validation manifest. */
  Validation?: ValidationData | null;
}
export function toTestingDataResult(root: jsonP.JSONValue): TestingDataResult {
  return jsonP.readObj({
    required: {},
    optional: {
      "Input": toTestingData,
      "Output": toTestingData,
      "Validation": toValidationData,
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** The evaluation results for the training of a model. */
export interface EvaluationResult {
  /** The F1 score for the evaluation of all labels. */
  F1Score?: number | null;
  /** The S3 bucket that contains the training summary. */
  Summary?: Summary | null;
}
export function toEvaluationResult(root: jsonP.JSONValue): EvaluationResult {
  return jsonP.readObj({
    required: {},
    optional: {
      "F1Score": "n",
      "Summary": toSummary,
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** The S3 bucket that contains the training summary. */
export interface Summary {
  S3Object?: S3Object | null;
}
export function toSummary(root: jsonP.JSONValue): Summary {
  return jsonP.readObj({
    required: {},
    optional: {
      "S3Object": toS3Object,
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** A description of an Amazon Rekognition Custom Labels project. */
export interface ProjectDescription {
  /** The Amazon Resource Name (ARN) of the project. */
  ProjectArn?: string | null;
  /** The Unix timestamp for the date and time that the project was created. */
  CreationTimestamp?: Date | number | null;
  /** The current status of the project. */
  Status?: ProjectStatus | null;
  /** Information about the training and test datasets in the project. */
  Datasets?: DatasetMetadata[] | null;
}
export function toProjectDescription(root: jsonP.JSONValue): ProjectDescription {
  return jsonP.readObj({
    required: {},
    optional: {
      "ProjectArn": "s",
      "CreationTimestamp": "d",
      "Status": (x: jsonP.JSONValue) => cmnP.readEnum<ProjectStatus>(x),
      "Datasets": [toDatasetMetadata],
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Summary information for an Amazon Rekognition Custom Labels dataset. */
export interface DatasetMetadata {
  /** The Unix timestamp for the date and time that the dataset was created. */
  CreationTimestamp?: Date | number | null;
  /** The type of the dataset. */
  DatasetType?: DatasetType | null;
  /** The Amazon Resource Name (ARN) for the dataset. */
  DatasetArn?: string | null;
  /** The status for the dataset. */
  Status?: DatasetStatus | null;
  /** The status message for the dataset. */
  StatusMessage?: string | null;
  /** The status message code for the dataset operation. */
  StatusMessageCode?: DatasetStatusMessageCode | null;
}
export function toDatasetMetadata(root: jsonP.JSONValue): DatasetMetadata {
  return jsonP.readObj({
    required: {},
    optional: {
      "CreationTimestamp": "d",
      "DatasetType": (x: jsonP.JSONValue) => cmnP.readEnum<DatasetType>(x),
      "DatasetArn": "s",
      "Status": (x: jsonP.JSONValue) => cmnP.readEnum<DatasetStatus>(x),
      "StatusMessage": "s",
      "StatusMessageCode": (x: jsonP.JSONValue) => cmnP.readEnum<DatasetStatusMessageCode>(x),
    },
  }, root);
}

// refs: 2 - tags: output, named, enum
export type StreamProcessorStatus =
| "STOPPED"
| "STARTING"
| "RUNNING"
| "FAILED"
| "STOPPING"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: output, named, interface
/** A custom label detected in an image by a call to "DetectCustomLabels". */
export interface CustomLabel {
  /** The name of the custom label. */
  Name?: string | null;
  /** The confidence that the model has in the detection of the custom label. */
  Confidence?: number | null;
  /** The location of the detected object on the image that corresponds to the custom label. */
  Geometry?: Geometry | null;
}
export function toCustomLabel(root: jsonP.JSONValue): CustomLabel {
  return jsonP.readObj({
    required: {},
    optional: {
      "Name": "s",
      "Confidence": "n",
      "Geometry": toGeometry,
    },
  }, root);
}

// refs: 3 - tags: output, named, interface
/** Information about where an object ("DetectCustomLabels") or text ("DetectText") is located on an image. */
export interface Geometry {
  /** An axis-aligned coarse representation of the detected item's location on the image. */
  BoundingBox?: BoundingBox | null;
  /** Within the bounding box, a fine-grained polygon around the detected item. */
  Polygon?: Point[] | null;
}
export function toGeometry(root: jsonP.JSONValue): Geometry {
  return jsonP.readObj({
    required: {},
    optional: {
      "BoundingBox": toBoundingBox,
      "Polygon": [toPoint],
    },
  }, root);
}

// refs: 3 - tags: output, named, interface
/** The X and Y coordinates of a point on an image. */
export interface Point {
  /** The value of the X coordinate for a point on a `Polygon`. */
  X?: number | null;
  /** The value of the Y coordinate for a point on a `Polygon`. */
  Y?: number | null;
}
export function toPoint(root: jsonP.JSONValue): Point {
  return jsonP.readObj({
    required: {},
    optional: {
      "X": "n",
      "Y": "n",
    },
  }, root);
}

// refs: 7 - tags: output, named, interface
/** Structure containing attributes of the face that the algorithm detected. */
export interface FaceDetail {
  /** Bounding box of the face. */
  BoundingBox?: BoundingBox | null;
  /** The estimated age range, in years, for the face. */
  AgeRange?: AgeRange | null;
  /** Indicates whether or not the face is smiling, and the confidence level in the determination. */
  Smile?: Smile | null;
  /** Indicates whether or not the face is wearing eye glasses, and the confidence level in the determination. */
  Eyeglasses?: Eyeglasses | null;
  /** Indicates whether or not the face is wearing sunglasses, and the confidence level in the determination. */
  Sunglasses?: Sunglasses | null;
  /** The predicted gender of a detected face. */
  Gender?: Gender | null;
  /** Indicates whether or not the face has a beard, and the confidence level in the determination. */
  Beard?: Beard | null;
  /** Indicates whether or not the face has a mustache, and the confidence level in the determination. */
  Mustache?: Mustache | null;
  /** Indicates whether or not the eyes on the face are open, and the confidence level in the determination. */
  EyesOpen?: EyeOpen | null;
  /** Indicates whether or not the mouth on the face is open, and the confidence level in the determination. */
  MouthOpen?: MouthOpen | null;
  /** The emotions that appear to be expressed on the face, and the confidence level in the determination. */
  Emotions?: Emotion[] | null;
  /** Indicates the location of landmarks on the face. */
  Landmarks?: Landmark[] | null;
  /** Indicates the pose of the face as determined by its pitch, roll, and yaw. */
  Pose?: Pose | null;
  /** Identifies image brightness and sharpness. */
  Quality?: ImageQuality | null;
  /** Confidence level that the bounding box contains a face (and not a different object such as a tree). */
  Confidence?: number | null;
}
export function toFaceDetail(root: jsonP.JSONValue): FaceDetail {
  return jsonP.readObj({
    required: {},
    optional: {
      "BoundingBox": toBoundingBox,
      "AgeRange": toAgeRange,
      "Smile": toSmile,
      "Eyeglasses": toEyeglasses,
      "Sunglasses": toSunglasses,
      "Gender": toGender,
      "Beard": toBeard,
      "Mustache": toMustache,
      "EyesOpen": toEyeOpen,
      "MouthOpen": toMouthOpen,
      "Emotions": [toEmotion],
      "Landmarks": [toLandmark],
      "Pose": toPose,
      "Quality": toImageQuality,
      "Confidence": "n",
    },
  }, root);
}

// refs: 7 - tags: output, named, interface
/** Structure containing the estimated age range, in years, for a face. */
export interface AgeRange {
  /** The lowest estimated age. */
  Low?: number | null;
  /** The highest estimated age. */
  High?: number | null;
}
export function toAgeRange(root: jsonP.JSONValue): AgeRange {
  return jsonP.readObj({
    required: {},
    optional: {
      "Low": "n",
      "High": "n",
    },
  }, root);
}

// refs: 7 - tags: output, named, interface
/** Indicates whether or not the face is wearing eye glasses, and the confidence level in the determination. */
export interface Eyeglasses {
  /** Boolean value that indicates whether the face is wearing eye glasses or not. */
  Value?: boolean | null;
  /** Level of confidence in the determination. */
  Confidence?: number | null;
}
export function toEyeglasses(root: jsonP.JSONValue): Eyeglasses {
  return jsonP.readObj({
    required: {},
    optional: {
      "Value": "b",
      "Confidence": "n",
    },
  }, root);
}

// refs: 7 - tags: output, named, interface
/** Indicates whether or not the face is wearing sunglasses, and the confidence level in the determination. */
export interface Sunglasses {
  /** Boolean value that indicates whether the face is wearing sunglasses or not. */
  Value?: boolean | null;
  /** Level of confidence in the determination. */
  Confidence?: number | null;
}
export function toSunglasses(root: jsonP.JSONValue): Sunglasses {
  return jsonP.readObj({
    required: {},
    optional: {
      "Value": "b",
      "Confidence": "n",
    },
  }, root);
}

// refs: 7 - tags: output, named, interface
/** The predicted gender of a detected face. */
export interface Gender {
  /** The predicted gender of the face. */
  Value?: GenderType | null;
  /** Level of confidence in the prediction. */
  Confidence?: number | null;
}
export function toGender(root: jsonP.JSONValue): Gender {
  return jsonP.readObj({
    required: {},
    optional: {
      "Value": (x: jsonP.JSONValue) => cmnP.readEnum<GenderType>(x),
      "Confidence": "n",
    },
  }, root);
}

// refs: 7 - tags: output, named, enum
export type GenderType =
| "Male"
| "Female"
| cmnP.UnexpectedEnumValue;

// refs: 7 - tags: output, named, interface
/** Indicates whether or not the face has a beard, and the confidence level in the determination. */
export interface Beard {
  /** Boolean value that indicates whether the face has beard or not. */
  Value?: boolean | null;
  /** Level of confidence in the determination. */
  Confidence?: number | null;
}
export function toBeard(root: jsonP.JSONValue): Beard {
  return jsonP.readObj({
    required: {},
    optional: {
      "Value": "b",
      "Confidence": "n",
    },
  }, root);
}

// refs: 7 - tags: output, named, interface
/** Indicates whether or not the face has a mustache, and the confidence level in the determination. */
export interface Mustache {
  /** Boolean value that indicates whether the face has mustache or not. */
  Value?: boolean | null;
  /** Level of confidence in the determination. */
  Confidence?: number | null;
}
export function toMustache(root: jsonP.JSONValue): Mustache {
  return jsonP.readObj({
    required: {},
    optional: {
      "Value": "b",
      "Confidence": "n",
    },
  }, root);
}

// refs: 7 - tags: output, named, interface
/** Indicates whether or not the eyes on the face are open, and the confidence level in the determination. */
export interface EyeOpen {
  /** Boolean value that indicates whether the eyes on the face are open. */
  Value?: boolean | null;
  /** Level of confidence in the determination. */
  Confidence?: number | null;
}
export function toEyeOpen(root: jsonP.JSONValue): EyeOpen {
  return jsonP.readObj({
    required: {},
    optional: {
      "Value": "b",
      "Confidence": "n",
    },
  }, root);
}

// refs: 7 - tags: output, named, interface
/** Indicates whether or not the mouth on the face is open, and the confidence level in the determination. */
export interface MouthOpen {
  /** Boolean value that indicates whether the mouth on the face is open or not. */
  Value?: boolean | null;
  /** Level of confidence in the determination. */
  Confidence?: number | null;
}
export function toMouthOpen(root: jsonP.JSONValue): MouthOpen {
  return jsonP.readObj({
    required: {},
    optional: {
      "Value": "b",
      "Confidence": "n",
    },
  }, root);
}

// refs: 2 - tags: output, named, interface
/** Structure containing details about the detected label, including the name, detected instances, parent labels, and level of confidence. */
export interface Label {
  /** The name (label) of the object or scene. */
  Name?: string | null;
  /** Level of confidence. */
  Confidence?: number | null;
  /** If `Label` represents an object, `Instances` contains the bounding boxes for each instance of the detected object. */
  Instances?: Instance[] | null;
  /** The parent labels for a label. */
  Parents?: Parent[] | null;
}
export function toLabel(root: jsonP.JSONValue): Label {
  return jsonP.readObj({
    required: {},
    optional: {
      "Name": "s",
      "Confidence": "n",
      "Instances": [toInstance],
      "Parents": [toParent],
    },
  }, root);
}

// refs: 2 - tags: output, named, interface
/** An instance of a label returned by Amazon Rekognition Image ("DetectLabels") or by Amazon Rekognition Video ("GetLabelDetection"). */
export interface Instance {
  /** The position of the label instance on the image. */
  BoundingBox?: BoundingBox | null;
  /** The confidence that Amazon Rekognition has in the accuracy of the bounding box. */
  Confidence?: number | null;
}
export function toInstance(root: jsonP.JSONValue): Instance {
  return jsonP.readObj({
    required: {},
    optional: {
      "BoundingBox": toBoundingBox,
      "Confidence": "n",
    },
  }, root);
}

// refs: 2 - tags: output, named, interface
/** A parent label for a label. */
export interface Parent {
  /** The name of the parent label. */
  Name?: string | null;
}
export function toParent(root: jsonP.JSONValue): Parent {
  return jsonP.readObj({
    required: {},
    optional: {
      "Name": "s",
    },
  }, root);
}

// refs: 2 - tags: output, named, interface
/** Provides information about a single type of inappropriate, unwanted, or offensive content found in an image or video. */
export interface ModerationLabel {
  /** Specifies the confidence that Amazon Rekognition has that the label has been correctly identified. */
  Confidence?: number | null;
  /** The label name for the type of unsafe content detected in the image. */
  Name?: string | null;
  /** The name for the parent label. */
  ParentName?: string | null;
}
export function toModerationLabel(root: jsonP.JSONValue): ModerationLabel {
  return jsonP.readObj({
    required: {},
    optional: {
      "Confidence": "n",
      "Name": "s",
      "ParentName": "s",
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Shows the results of the human in the loop evaluation. */
export interface HumanLoopActivationOutput {
  /** The Amazon Resource Name (ARN) of the HumanLoop created. */
  HumanLoopArn?: string | null;
  /** Shows if and why human review was needed. */
  HumanLoopActivationReasons?: string[] | null;
  /** Shows the result of condition evaluations, including those conditions which activated a human review. */
  HumanLoopActivationConditionsEvaluationResults?: jsonP.JSONValue | null;
}
export function toHumanLoopActivationOutput(root: jsonP.JSONValue): HumanLoopActivationOutput {
  return jsonP.readObj({
    required: {},
    optional: {
      "HumanLoopArn": "s",
      "HumanLoopActivationReasons": ["s"],
      "HumanLoopActivationConditionsEvaluationResults": jsonP.readJsonValue,
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** A person detected by a call to "DetectProtectiveEquipment". */
export interface ProtectiveEquipmentPerson {
  /** An array of body parts detected on a person's body (including body parts without PPE). */
  BodyParts?: ProtectiveEquipmentBodyPart[] | null;
  /** A bounding box around the detected person. */
  BoundingBox?: BoundingBox | null;
  /** The confidence that Amazon Rekognition has that the bounding box contains a person. */
  Confidence?: number | null;
  /** The identifier for the detected person. */
  Id?: number | null;
}
export function toProtectiveEquipmentPerson(root: jsonP.JSONValue): ProtectiveEquipmentPerson {
  return jsonP.readObj({
    required: {},
    optional: {
      "BodyParts": [toProtectiveEquipmentBodyPart],
      "BoundingBox": toBoundingBox,
      "Confidence": "n",
      "Id": "n",
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Information about a body part detected by "DetectProtectiveEquipment" that contains PPE. */
export interface ProtectiveEquipmentBodyPart {
  /** The detected body part. */
  Name?: BodyPart | null;
  /** The confidence that Amazon Rekognition has in the detection accuracy of the detected body part. */
  Confidence?: number | null;
  /** An array of Personal Protective Equipment items detected around a body part. */
  EquipmentDetections?: EquipmentDetection[] | null;
}
export function toProtectiveEquipmentBodyPart(root: jsonP.JSONValue): ProtectiveEquipmentBodyPart {
  return jsonP.readObj({
    required: {},
    optional: {
      "Name": (x: jsonP.JSONValue) => cmnP.readEnum<BodyPart>(x),
      "Confidence": "n",
      "EquipmentDetections": [toEquipmentDetection],
    },
  }, root);
}

// refs: 1 - tags: output, named, enum
export type BodyPart =
| "FACE"
| "HEAD"
| "LEFT_HAND"
| "RIGHT_HAND"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: output, named, interface
/** Information about an item of Personal Protective Equipment (PPE) detected by "DetectProtectiveEquipment". */
export interface EquipmentDetection {
  /** A bounding box surrounding the item of detected PPE. */
  BoundingBox?: BoundingBox | null;
  /** The confidence that Amazon Rekognition has that the bounding box (`BoundingBox`) contains an item of PPE. */
  Confidence?: number | null;
  /** The type of detected PPE. */
  Type?: ProtectiveEquipmentType | null;
  /** Information about the body part covered by the detected PPE. */
  CoversBodyPart?: CoversBodyPart | null;
}
export function toEquipmentDetection(root: jsonP.JSONValue): EquipmentDetection {
  return jsonP.readObj({
    required: {},
    optional: {
      "BoundingBox": toBoundingBox,
      "Confidence": "n",
      "Type": (x: jsonP.JSONValue) => cmnP.readEnum<ProtectiveEquipmentType>(x),
      "CoversBodyPart": toCoversBodyPart,
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Information about an item of Personal Protective Equipment covering a corresponding body part. */
export interface CoversBodyPart {
  /** The confidence that Amazon Rekognition has in the value of `Value`. */
  Confidence?: number | null;
  /** True if the PPE covers the corresponding body part, otherwise false. */
  Value?: boolean | null;
}
export function toCoversBodyPart(root: jsonP.JSONValue): CoversBodyPart {
  return jsonP.readObj({
    required: {},
    optional: {
      "Confidence": "n",
      "Value": "b",
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Summary information for required items of personal protective equipment (PPE) detected on persons by a call to "DetectProtectiveEquipment". */
export interface ProtectiveEquipmentSummary {
  /** An array of IDs for persons who are wearing detected personal protective equipment. */
  PersonsWithRequiredEquipment?: number[] | null;
  /** An array of IDs for persons who are not wearing all of the types of PPE specified in the `RequiredEquipmentTypes` field of the detected personal protective equipment. */
  PersonsWithoutRequiredEquipment?: number[] | null;
  /** An array of IDs for persons where it was not possible to determine if they are wearing personal protective equipment. */
  PersonsIndeterminate?: number[] | null;
}
export function toProtectiveEquipmentSummary(root: jsonP.JSONValue): ProtectiveEquipmentSummary {
  return jsonP.readObj({
    required: {},
    optional: {
      "PersonsWithRequiredEquipment": ["n"],
      "PersonsWithoutRequiredEquipment": ["n"],
      "PersonsIndeterminate": ["n"],
    },
  }, root);
}

// refs: 2 - tags: output, named, interface
/** Information about a word or line of text detected by "DetectText". */
export interface TextDetection {
  /** The word or line of text recognized by Amazon Rekognition. */
  DetectedText?: string | null;
  /** The type of text that was detected. */
  Type?: TextTypes | null;
  /** The identifier for the detected text. */
  Id?: number | null;
  /** The Parent identifier for the detected text identified by the value of `ID`. */
  ParentId?: number | null;
  /** The confidence that Amazon Rekognition has in the accuracy of the detected text and the accuracy of the geometry points around the detected text. */
  Confidence?: number | null;
  /** The location of the detected text on the image. */
  Geometry?: Geometry | null;
}
export function toTextDetection(root: jsonP.JSONValue): TextDetection {
  return jsonP.readObj({
    required: {},
    optional: {
      "DetectedText": "s",
      "Type": (x: jsonP.JSONValue) => cmnP.readEnum<TextTypes>(x),
      "Id": "n",
      "ParentId": "n",
      "Confidence": "n",
      "Geometry": toGeometry,
    },
  }, root);
}

// refs: 2 - tags: output, named, enum
export type TextTypes =
| "LINE"
| "WORD"
| cmnP.UnexpectedEnumValue;

// refs: 3 - tags: output, named, interface
/** The known gender identity for the celebrity that matches the provided ID. */
export interface KnownGender {
  /** A string value of the KnownGender info about the Celebrity. */
  Type?: KnownGenderType | null;
}
export function toKnownGender(root: jsonP.JSONValue): KnownGender {
  return jsonP.readObj({
    required: {},
    optional: {
      "Type": (x: jsonP.JSONValue) => cmnP.readEnum<KnownGenderType>(x),
    },
  }, root);
}

// refs: 3 - tags: output, named, enum
/** A list of enum string of possible gender values that Celebrity returns. */
export type KnownGenderType =
| "Male"
| "Female"
| "Nonbinary"
| "Unlisted"
| cmnP.UnexpectedEnumValue;

// refs: 8 - tags: output, named, enum
export type VideoJobStatus =
| "IN_PROGRESS"
| "SUCCEEDED"
| "FAILED"
| cmnP.UnexpectedEnumValue;

// refs: 8 - tags: output, named, interface
/** Information about a video that Amazon Rekognition analyzed. */
export interface VideoMetadata {
  /** Type of compression used in the analyzed video. */
  Codec?: string | null;
  /** Length of the video in milliseconds. */
  DurationMillis?: number | null;
  /** Format of the analyzed video. */
  Format?: string | null;
  /** Number of frames per second in the video. */
  FrameRate?: number | null;
  /** Vertical pixel dimension of the video. */
  FrameHeight?: number | null;
  /** Horizontal pixel dimension of the video. */
  FrameWidth?: number | null;
  /** A description of the range of luminance values in a video, either LIMITED (16 to 235) or FULL (0 to 255). */
  ColorRange?: VideoColorRange | null;
}
export function toVideoMetadata(root: jsonP.JSONValue): VideoMetadata {
  return jsonP.readObj({
    required: {},
    optional: {
      "Codec": "s",
      "DurationMillis": "n",
      "Format": "s",
      "FrameRate": "n",
      "FrameHeight": "n",
      "FrameWidth": "n",
      "ColorRange": (x: jsonP.JSONValue) => cmnP.readEnum<VideoColorRange>(x),
    },
  }, root);
}

// refs: 8 - tags: output, named, enum
export type VideoColorRange =
| "FULL"
| "LIMITED"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: output, named, interface
/** Information about a detected celebrity and the time the celebrity was detected in a stored video. */
export interface CelebrityRecognition {
  /** The time, in milliseconds from the start of the video, that the celebrity was recognized. */
  Timestamp?: number | null;
  /** Information about a recognized celebrity. */
  Celebrity?: CelebrityDetail | null;
}
export function toCelebrityRecognition(root: jsonP.JSONValue): CelebrityRecognition {
  return jsonP.readObj({
    required: {},
    optional: {
      "Timestamp": "n",
      "Celebrity": toCelebrityDetail,
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Information about a recognized celebrity. */
export interface CelebrityDetail {
  /** An array of URLs pointing to additional celebrity information. */
  Urls?: string[] | null;
  /** The name of the celebrity. */
  Name?: string | null;
  /** The unique identifier for the celebrity. */
  Id?: string | null;
  /** The confidence, in percentage, that Amazon Rekognition has that the recognized face is the celebrity. */
  Confidence?: number | null;
  /** Bounding box around the body of a celebrity. */
  BoundingBox?: BoundingBox | null;
  /** Face details for the recognized celebrity. */
  Face?: FaceDetail | null;
  /** Retrieves the known gender for the celebrity. */
  KnownGender?: KnownGender | null;
}
export function toCelebrityDetail(root: jsonP.JSONValue): CelebrityDetail {
  return jsonP.readObj({
    required: {},
    optional: {
      "Urls": ["s"],
      "Name": "s",
      "Id": "s",
      "Confidence": "n",
      "BoundingBox": toBoundingBox,
      "Face": toFaceDetail,
      "KnownGender": toKnownGender,
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Information about an inappropriate, unwanted, or offensive content label detection in a stored video. */
export interface ContentModerationDetection {
  /** Time, in milliseconds from the beginning of the video, that the content moderation label was detected. */
  Timestamp?: number | null;
  /** The content moderation label detected by in the stored video. */
  ModerationLabel?: ModerationLabel | null;
}
export function toContentModerationDetection(root: jsonP.JSONValue): ContentModerationDetection {
  return jsonP.readObj({
    required: {},
    optional: {
      "Timestamp": "n",
      "ModerationLabel": toModerationLabel,
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Information about a face detected in a video analysis request and the time the face was detected in the video. */
export interface FaceDetection {
  /** Time, in milliseconds from the start of the video, that the face was detected. */
  Timestamp?: number | null;
  /** The face properties for the detected face. */
  Face?: FaceDetail | null;
}
export function toFaceDetection(root: jsonP.JSONValue): FaceDetection {
  return jsonP.readObj({
    required: {},
    optional: {
      "Timestamp": "n",
      "Face": toFaceDetail,
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Information about a person whose face matches a face(s) in an Amazon Rekognition collection. */
export interface PersonMatch {
  /** The time, in milliseconds from the beginning of the video, that the person was matched in the video. */
  Timestamp?: number | null;
  /** Information about the matched person. */
  Person?: PersonDetail | null;
  /** Information about the faces in the input collection that match the face of a person in the video. */
  FaceMatches?: FaceMatch[] | null;
}
export function toPersonMatch(root: jsonP.JSONValue): PersonMatch {
  return jsonP.readObj({
    required: {},
    optional: {
      "Timestamp": "n",
      "Person": toPersonDetail,
      "FaceMatches": [toFaceMatch],
    },
  }, root);
}

// refs: 2 - tags: output, named, interface
/** Details about a person detected in a video analysis request. */
export interface PersonDetail {
  /** Identifier for the person detected person within a video. */
  Index?: number | null;
  /** Bounding box around the detected person. */
  BoundingBox?: BoundingBox | null;
  /** Face details for the detected person. */
  Face?: FaceDetail | null;
}
export function toPersonDetail(root: jsonP.JSONValue): PersonDetail {
  return jsonP.readObj({
    required: {},
    optional: {
      "Index": "n",
      "BoundingBox": toBoundingBox,
      "Face": toFaceDetail,
    },
  }, root);
}

// refs: 3 - tags: output, named, interface
/** Provides face metadata. */
export interface FaceMatch {
  /** Confidence in the match of this face with the input face. */
  Similarity?: number | null;
  /** Describes the face properties such as the bounding box, face ID, image ID of the source image, and external image ID that you assigned. */
  Face?: Face | null;
}
export function toFaceMatch(root: jsonP.JSONValue): FaceMatch {
  return jsonP.readObj({
    required: {},
    optional: {
      "Similarity": "n",
      "Face": toFace,
    },
  }, root);
}

// refs: 5 - tags: output, named, interface
/** Describes the face properties such as the bounding box, face ID, image ID of the input image, and external image ID that you assigned. */
export interface Face {
  /** Unique identifier that Amazon Rekognition assigns to the face. */
  FaceId?: string | null;
  /** Bounding box of the face. */
  BoundingBox?: BoundingBox | null;
  /** Unique identifier that Amazon Rekognition assigns to the input image. */
  ImageId?: string | null;
  /** Identifier that you assign to all the faces in the input image. */
  ExternalImageId?: string | null;
  /** Confidence level that the bounding box contains a face (and not a different object such as a tree). */
  Confidence?: number | null;
  /** The version of the face detect and storage model that was used when indexing the face vector. */
  IndexFacesModelVersion?: string | null;
}
export function toFace(root: jsonP.JSONValue): Face {
  return jsonP.readObj({
    required: {},
    optional: {
      "FaceId": "s",
      "BoundingBox": toBoundingBox,
      "ImageId": "s",
      "ExternalImageId": "s",
      "Confidence": "n",
      "IndexFacesModelVersion": "s",
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Information about a label detected in a video analysis request and the time the label was detected in the video. */
export interface LabelDetection {
  /** Time, in milliseconds from the start of the video, that the label was detected. */
  Timestamp?: number | null;
  /** Details about the detected label. */
  Label?: Label | null;
}
export function toLabelDetection(root: jsonP.JSONValue): LabelDetection {
  return jsonP.readObj({
    required: {},
    optional: {
      "Timestamp": "n",
      "Label": toLabel,
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Details and path tracking information for a single time a person's path is tracked in a video. */
export interface PersonDetection {
  /** The time, in milliseconds from the start of the video, that the person's path was tracked. */
  Timestamp?: number | null;
  /** Details about a person whose path was tracked in a video. */
  Person?: PersonDetail | null;
}
export function toPersonDetection(root: jsonP.JSONValue): PersonDetection {
  return jsonP.readObj({
    required: {},
    optional: {
      "Timestamp": "n",
      "Person": toPersonDetail,
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Metadata information about an audio stream. */
export interface AudioMetadata {
  /** The audio codec used to encode or decode the audio stream. */
  Codec?: string | null;
  /** The duration of the audio stream in milliseconds. */
  DurationMillis?: number | null;
  /** The sample rate for the audio stream. */
  SampleRate?: number | null;
  /** The number of audio channels in the segment. */
  NumberOfChannels?: number | null;
}
export function toAudioMetadata(root: jsonP.JSONValue): AudioMetadata {
  return jsonP.readObj({
    required: {},
    optional: {
      "Codec": "s",
      "DurationMillis": "n",
      "SampleRate": "n",
      "NumberOfChannels": "n",
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** A technical cue or shot detection segment detected in a video. */
export interface SegmentDetection {
  /** The type of the segment. */
  Type?: SegmentType | null;
  /** The start time of the detected segment in milliseconds from the start of the video. */
  StartTimestampMillis?: number | null;
  /** The end time of the detected segment, in milliseconds, from the start of the video. */
  EndTimestampMillis?: number | null;
  /** The duration of the detected segment in milliseconds. */
  DurationMillis?: number | null;
  /** The frame-accurate SMPTE timecode, from the start of a video, for the start of a detected segment. */
  StartTimecodeSMPTE?: string | null;
  /** The frame-accurate SMPTE timecode, from the start of a video, for the end of a detected segment. */
  EndTimecodeSMPTE?: string | null;
  /** The duration of the timecode for the detected segment in SMPTE format. */
  DurationSMPTE?: string | null;
  /** If the segment is a technical cue, contains information about the technical cue. */
  TechnicalCueSegment?: TechnicalCueSegment | null;
  /** If the segment is a shot detection, contains information about the shot detection. */
  ShotSegment?: ShotSegment | null;
  /** The frame number of the start of a video segment, using a frame index that starts with 0. */
  StartFrameNumber?: number | null;
  /** The frame number at the end of a video segment, using a frame index that starts with 0. */
  EndFrameNumber?: number | null;
  /** The duration of a video segment, expressed in frames. */
  DurationFrames?: number | null;
}
export function toSegmentDetection(root: jsonP.JSONValue): SegmentDetection {
  return jsonP.readObj({
    required: {},
    optional: {
      "Type": (x: jsonP.JSONValue) => cmnP.readEnum<SegmentType>(x),
      "StartTimestampMillis": "n",
      "EndTimestampMillis": "n",
      "DurationMillis": "n",
      "StartTimecodeSMPTE": "s",
      "EndTimecodeSMPTE": "s",
      "DurationSMPTE": "s",
      "TechnicalCueSegment": toTechnicalCueSegment,
      "ShotSegment": toShotSegment,
      "StartFrameNumber": "n",
      "EndFrameNumber": "n",
      "DurationFrames": "n",
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Information about a technical cue segment. */
export interface TechnicalCueSegment {
  /** The type of the technical cue. */
  Type?: TechnicalCueType | null;
  /** The confidence that Amazon Rekognition Video has in the accuracy of the detected segment. */
  Confidence?: number | null;
}
export function toTechnicalCueSegment(root: jsonP.JSONValue): TechnicalCueSegment {
  return jsonP.readObj({
    required: {},
    optional: {
      "Type": (x: jsonP.JSONValue) => cmnP.readEnum<TechnicalCueType>(x),
      "Confidence": "n",
    },
  }, root);
}

// refs: 1 - tags: output, named, enum
export type TechnicalCueType =
| "ColorBars"
| "EndCredits"
| "BlackFrames"
| "OpeningCredits"
| "StudioLogo"
| "Slate"
| "Content"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: output, named, interface
/** Information about a shot detection segment detected in a video. */
export interface ShotSegment {
  /** An Identifier for a shot detection segment detected in a video. */
  Index?: number | null;
  /** The confidence that Amazon Rekognition Video has in the accuracy of the detected segment. */
  Confidence?: number | null;
}
export function toShotSegment(root: jsonP.JSONValue): ShotSegment {
  return jsonP.readObj({
    required: {},
    optional: {
      "Index": "n",
      "Confidence": "n",
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Information about the type of a segment requested in a call to "StartSegmentDetection". */
export interface SegmentTypeInfo {
  /** The type of a segment (technical cue or shot detection). */
  Type?: SegmentType | null;
  /** The version of the model used to detect segments. */
  ModelVersion?: string | null;
}
export function toSegmentTypeInfo(root: jsonP.JSONValue): SegmentTypeInfo {
  return jsonP.readObj({
    required: {},
    optional: {
      "Type": (x: jsonP.JSONValue) => cmnP.readEnum<SegmentType>(x),
      "ModelVersion": "s",
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Information about text detected in a video. */
export interface TextDetectionResult {
  /** The time, in milliseconds from the start of the video, that the text was detected. */
  Timestamp?: number | null;
  /** Details about text detected in a video. */
  TextDetection?: TextDetection | null;
}
export function toTextDetectionResult(root: jsonP.JSONValue): TextDetectionResult {
  return jsonP.readObj({
    required: {},
    optional: {
      "Timestamp": "n",
      "TextDetection": toTextDetection,
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Object containing both the face metadata (stored in the backend database), and facial attributes that are detected but aren't stored in the database. */
export interface FaceRecord {
  /** Describes the face properties such as the bounding box, face ID, image ID of the input image, and external image ID that you assigned. */
  Face?: Face | null;
  /** Structure containing attributes of the face that the algorithm detected. */
  FaceDetail?: FaceDetail | null;
}
export function toFaceRecord(root: jsonP.JSONValue): FaceRecord {
  return jsonP.readObj({
    required: {},
    optional: {
      "Face": toFace,
      "FaceDetail": toFaceDetail,
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** A face that "IndexFaces" detected, but didn't index. */
export interface UnindexedFace {
  /** An array of reasons that specify why a face wasn't indexed. */
  Reasons?: Reason[] | null;
  /** The structure that contains attributes of a face that `IndexFaces`detected, but didn't index. */
  FaceDetail?: FaceDetail | null;
}
export function toUnindexedFace(root: jsonP.JSONValue): UnindexedFace {
  return jsonP.readObj({
    required: {},
    optional: {
      "Reasons": [(x: jsonP.JSONValue) => cmnP.readEnum<Reason>(x)],
      "FaceDetail": toFaceDetail,
    },
  }, root);
}

// refs: 1 - tags: output, named, enum
export type Reason =
| "EXCEEDS_MAX_FACES"
| "EXTREME_POSE"
| "LOW_BRIGHTNESS"
| "LOW_SHARPNESS"
| "LOW_CONFIDENCE"
| "SMALL_BOUNDING_BOX"
| "LOW_FACE_QUALITY"
| cmnP.UnexpectedEnumValue;

// refs: 1 - tags: output, named, interface
/** Describes a dataset label. */
export interface DatasetLabelDescription {
  /** The name of the label. */
  LabelName?: string | null;
  /** Statistics about the label. */
  LabelStats?: DatasetLabelStats | null;
}
export function toDatasetLabelDescription(root: jsonP.JSONValue): DatasetLabelDescription {
  return jsonP.readObj({
    required: {},
    optional: {
      "LabelName": "s",
      "LabelStats": toDatasetLabelStats,
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Statistics about a label used in a dataset. */
export interface DatasetLabelStats {
  /** The total number of images that use the label. */
  EntryCount?: number | null;
  /** The total number of images that have the label assigned to a bounding box. */
  BoundingBoxCount?: number | null;
}
export function toDatasetLabelStats(root: jsonP.JSONValue): DatasetLabelStats {
  return jsonP.readObj({
    required: {},
    optional: {
      "EntryCount": "n",
      "BoundingBoxCount": "n",
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** An object that recognizes faces in a streaming video. */
export interface StreamProcessor {
  /** Name of the Amazon Rekognition stream processor. */
  Name?: string | null;
  /** Current status of the Amazon Rekognition stream processor. */
  Status?: StreamProcessorStatus | null;
}
export function toStreamProcessor(root: jsonP.JSONValue): StreamProcessor {
  return jsonP.readObj({
    required: {},
    optional: {
      "Name": "s",
      "Status": (x: jsonP.JSONValue) => cmnP.readEnum<StreamProcessorStatus>(x),
    },
  }, root);
}

// refs: 1 - tags: output, named, interface
/** Provides information about a celebrity recognized by the "RecognizeCelebrities" operation. */
export interface Celebrity {
  /** An array of URLs pointing to additional information about the celebrity. */
  Urls?: string[] | null;
  /** The name of the celebrity. */
  Name?: string | null;
  /** A unique identifier for the celebrity. */
  Id?: string | null;
  /** Provides information about the celebrity's face, such as its location on the image. */
  Face?: ComparedFace | null;
  /** The confidence, in percentage, that Amazon Rekognition has that the recognized face is the celebrity. */
  MatchConfidence?: number | null;
  KnownGender?: KnownGender | null;
}
export function toCelebrity(root: jsonP.JSONValue): Celebrity {
  return jsonP.readObj({
    required: {},
    optional: {
      "Urls": ["s"],
      "Name": "s",
      "Id": "s",
      "Face": toComparedFace,
      "MatchConfidence": "n",
      "KnownGender": toKnownGender,
    },
  }, root);
}
